{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "• Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "• You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "✓ Activation function: tanh, ReLU\n",
    "\n",
    "✓ Data preprocessing\n",
    "\n",
    "✓ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "✓ Loss function: without or with the regularization term \n",
    "(L2), λ = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[𝑓(X^c, w) −y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "✓ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "✓ Learning epochs: 100, 200, 300\n",
    "\n",
    "✓ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "✓ Learning rate decay schedule: none and cosine\n",
    "\n",
    "✓ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVES = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "INIT_FUNCS = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "OPTIM_FUNCS = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr, weight_decay: optim.SGD(params=param, lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "SCHEDULERS = {\n",
    "    \"cos\": lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay:float):\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = 100. * val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'\n",
    "              .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
    "        print(val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData():\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=False, transform=transforms.transforms.ToTensor())\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=False, transform=transforms.transforms.ToTensor())\n",
    "    # Split the training set into training and validation sets\n",
    "    train_count = int(0.8 * len(trainset))\n",
    "    valid_count = len(trainset) - train_count\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset = random_split(\n",
    "        trainset, (train_count, valid_count), Generator().manual_seed(42))\n",
    "    # Create data loaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # 19\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        # 5\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            setattr(self, f\"labels_of_{col}\", d.keys())\n",
    "            reservations[col]=reservations[col].map(d.__getitem__)\n",
    "        # 17(drop id)\n",
    "        self.feature = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.booking_status = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.feature),))\n",
    "        self.classes = list(getattr(self, f\"labels_of_{reservations.columns[-1]}\"))\n",
    "    def __len__(self):\n",
    "        return len(self.booking_status)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.booking_status[idx]\n",
    "\n",
    "# kaggle: ahsan81/hotel-reservations-classification-dataset\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = HotelReservationDataset(\n",
    "        csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\")\n",
    "    class_amount = len(dataset.classes)\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    # set loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data(zipped csv) from kaggle with username and apikey\n",
    "import os\n",
    "import json\n",
    "with open(\"kaggle.json\", \"r\") as j:\n",
    "    for (k, v) in json.load(j).items():\n",
    "        os.environ[k] = v\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "# https://www.kaggle.com/datasets/uciml/iris/download?datasetVersionNumber=2\n",
    "# owner/datasetname\n",
    "api.dataset_download_files('uciml/iris', path=\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 100 small_random relu sgd cos\n",
      "5 100 small_random relu momentum cos\n",
      "5 100 small_random relu adam cos\n",
      "5 100 small_random tanh sgd cos\n",
      "5 100 small_random tanh momentum cos\n",
      "5 100 small_random tanh adam cos\n",
      "5 100 xavier relu sgd cos\n",
      "5 100 xavier relu momentum cos\n",
      "5 100 xavier relu adam cos\n",
      "5 100 xavier tanh sgd cos\n",
      "5 100 xavier tanh momentum cos\n",
      "5 100 xavier tanh adam cos\n",
      "5 100 kaiming relu sgd cos\n",
      "5 100 kaiming relu momentum cos\n",
      "5 100 kaiming relu adam cos\n",
      "5 100 kaiming tanh sgd cos\n",
      "5 100 kaiming tanh momentum cos\n",
      "5 100 kaiming tanh adam cos\n",
      "5 200 small_random relu sgd cos\n",
      "5 200 small_random relu momentum cos\n",
      "5 200 small_random relu adam cos\n",
      "5 200 small_random tanh sgd cos\n",
      "5 200 small_random tanh momentum cos\n",
      "5 200 small_random tanh adam cos\n",
      "5 200 xavier relu sgd cos\n",
      "5 200 xavier relu momentum cos\n",
      "5 200 xavier relu adam cos\n",
      "5 200 xavier tanh sgd cos\n",
      "5 200 xavier tanh momentum cos\n",
      "5 200 xavier tanh adam cos\n",
      "5 200 kaiming relu sgd cos\n",
      "5 200 kaiming relu momentum cos\n",
      "5 200 kaiming relu adam cos\n",
      "5 200 kaiming tanh sgd cos\n",
      "5 200 kaiming tanh momentum cos\n",
      "5 200 kaiming tanh adam cos\n",
      "5 300 small_random relu sgd cos\n",
      "5 300 small_random relu momentum cos\n",
      "5 300 small_random relu adam cos\n",
      "5 300 small_random tanh sgd cos\n",
      "5 300 small_random tanh momentum cos\n",
      "5 300 small_random tanh adam cos\n",
      "5 300 xavier relu sgd cos\n",
      "5 300 xavier relu momentum cos\n",
      "5 300 xavier relu adam cos\n",
      "5 300 xavier tanh sgd cos\n",
      "5 300 xavier tanh momentum cos\n",
      "5 300 xavier tanh adam cos\n",
      "5 300 kaiming relu sgd cos\n",
      "5 300 kaiming relu momentum cos\n",
      "5 300 kaiming relu adam cos\n",
      "5 300 kaiming tanh sgd cos\n",
      "5 300 kaiming tanh momentum cos\n",
      "5 300 kaiming tanh adam cos\n",
      "8 100 small_random relu sgd cos\n",
      "8 100 small_random relu momentum cos\n",
      "8 100 small_random relu adam cos\n",
      "8 100 small_random tanh sgd cos\n",
      "8 100 small_random tanh momentum cos\n",
      "8 100 small_random tanh adam cos\n",
      "8 100 xavier relu sgd cos\n",
      "8 100 xavier relu momentum cos\n",
      "8 100 xavier relu adam cos\n",
      "8 100 xavier tanh sgd cos\n",
      "8 100 xavier tanh momentum cos\n",
      "8 100 xavier tanh adam cos\n",
      "8 100 kaiming relu sgd cos\n",
      "8 100 kaiming relu momentum cos\n",
      "8 100 kaiming relu adam cos\n",
      "8 100 kaiming tanh sgd cos\n",
      "8 100 kaiming tanh momentum cos\n",
      "8 100 kaiming tanh adam cos\n",
      "8 200 small_random relu sgd cos\n",
      "8 200 small_random relu momentum cos\n",
      "8 200 small_random relu adam cos\n",
      "8 200 small_random tanh sgd cos\n",
      "8 200 small_random tanh momentum cos\n",
      "8 200 small_random tanh adam cos\n",
      "8 200 xavier relu sgd cos\n",
      "8 200 xavier relu momentum cos\n",
      "8 200 xavier relu adam cos\n",
      "8 200 xavier tanh sgd cos\n",
      "8 200 xavier tanh momentum cos\n",
      "8 200 xavier tanh adam cos\n",
      "8 200 kaiming relu sgd cos\n",
      "8 200 kaiming relu momentum cos\n",
      "8 200 kaiming relu adam cos\n",
      "8 200 kaiming tanh sgd cos\n",
      "8 200 kaiming tanh momentum cos\n",
      "8 200 kaiming tanh adam cos\n",
      "8 300 small_random relu sgd cos\n",
      "8 300 small_random relu momentum cos\n",
      "8 300 small_random relu adam cos\n",
      "8 300 small_random tanh sgd cos\n",
      "8 300 small_random tanh momentum cos\n",
      "8 300 small_random tanh adam cos\n",
      "8 300 xavier relu sgd cos\n",
      "8 300 xavier relu momentum cos\n",
      "8 300 xavier relu adam cos\n",
      "8 300 xavier tanh sgd cos\n",
      "8 300 xavier tanh momentum cos\n",
      "8 300 xavier tanh adam cos\n",
      "8 300 kaiming relu sgd cos\n",
      "8 300 kaiming relu momentum cos\n",
      "8 300 kaiming relu adam cos\n",
      "8 300 kaiming tanh sgd cos\n",
      "8 300 kaiming tanh momentum cos\n",
      "8 300 kaiming tanh adam cos\n",
      "11 100 small_random relu sgd cos\n",
      "11 100 small_random relu momentum cos\n",
      "11 100 small_random relu adam cos\n",
      "11 100 small_random tanh sgd cos\n",
      "11 100 small_random tanh momentum cos\n",
      "11 100 small_random tanh adam cos\n",
      "11 100 xavier relu sgd cos\n",
      "11 100 xavier relu momentum cos\n",
      "11 100 xavier relu adam cos\n",
      "11 100 xavier tanh sgd cos\n",
      "11 100 xavier tanh momentum cos\n",
      "11 100 xavier tanh adam cos\n",
      "11 100 kaiming relu sgd cos\n",
      "11 100 kaiming relu momentum cos\n",
      "11 100 kaiming relu adam cos\n",
      "11 100 kaiming tanh sgd cos\n",
      "11 100 kaiming tanh momentum cos\n",
      "11 100 kaiming tanh adam cos\n",
      "11 200 small_random relu sgd cos\n",
      "11 200 small_random relu momentum cos\n",
      "11 200 small_random relu adam cos\n",
      "11 200 small_random tanh sgd cos\n",
      "11 200 small_random tanh momentum cos\n",
      "11 200 small_random tanh adam cos\n",
      "11 200 xavier relu sgd cos\n",
      "11 200 xavier relu momentum cos\n",
      "11 200 xavier relu adam cos\n",
      "11 200 xavier tanh sgd cos\n",
      "11 200 xavier tanh momentum cos\n",
      "11 200 xavier tanh adam cos\n",
      "11 200 kaiming relu sgd cos\n",
      "11 200 kaiming relu momentum cos\n",
      "11 200 kaiming relu adam cos\n",
      "11 200 kaiming tanh sgd cos\n",
      "11 200 kaiming tanh momentum cos\n",
      "11 200 kaiming tanh adam cos\n",
      "11 300 small_random relu sgd cos\n",
      "11 300 small_random relu momentum cos\n",
      "11 300 small_random relu adam cos\n",
      "11 300 small_random tanh sgd cos\n",
      "11 300 small_random tanh momentum cos\n",
      "11 300 small_random tanh adam cos\n",
      "11 300 xavier relu sgd cos\n",
      "11 300 xavier relu momentum cos\n",
      "11 300 xavier relu adam cos\n",
      "11 300 xavier tanh sgd cos\n",
      "11 300 xavier tanh momentum cos\n",
      "11 300 xavier tanh adam cos\n",
      "11 300 kaiming relu sgd cos\n",
      "11 300 kaiming relu momentum cos\n",
      "11 300 kaiming relu adam cos\n",
      "11 300 kaiming tanh sgd cos\n",
      "11 300 kaiming tanh momentum cos\n",
      "11 300 kaiming tanh adam cos\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def training_schedule():\n",
    "    sys.stdout = open(\"./result/\", \"w\")\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # ✓ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in INIT_FUNCS.items():\n",
    "                for (active, func) in ACTIVES.items():\n",
    "                    # ✓ Activation function: tanh, ReLU\n",
    "                    # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "                        # ✓ Learning rate decay schedule: none and cosine\n",
    "                        for (schedule, schd) in SCHEDULERS.items():\n",
    "                            # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "                            for weight_decay in (0.0, 0.001, 0.0001):\n",
    "                                print(hidden_size, epochs, init, active,optimize, schedule, \"start\")\n",
    "                                train(model=model, optm=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                                      trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "                                test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _training_schedule():\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #\n",
    "    hidden_size = 5\n",
    "    epochs = 100\n",
    "    init = \"small_random\"\n",
    "    method = INIT_FUNCS[init]\n",
    "    active = \"relu\"\n",
    "    func = ACTIVES[active]\n",
    "    optimize = \"sgd\"\n",
    "    optm = OPTIM_FUNCS[optimize]\n",
    "    schedule = None\n",
    "    schd = schedule\n",
    "    weight_decay = 0.0\n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "    # ✓ Learning epochs: 100, 200, 300\n",
    "    for epochs in (100, 200, 300):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        # Create model, optimizer, scheduler\n",
    "    for (init, method) in INIT_FUNCS.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "    for (active, func) in ACTIVES.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        # ✓ Activation function: tanh, ReLU\n",
    "        # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        # ✓ Learning rate decay schedule: none and cosine\n",
    "    for (schedule, schd) in SCHEDULERS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "    for weight_decay in (0.0, 0.001, 0.0001):\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000 10000\n",
      "5 100 small_random relu sgd None 0.0 start\n",
      "Epoch [1/100], Train Loss: 2.0659, Train Accuracy: 21.77%, Val Loss: 1.9020, Val Accuracy: 37.05%\n",
      "Epoch [2/100], Train Loss: 1.7745, Train Accuracy: 44.15%, Val Loss: 1.6487, Val Accuracy: 45.99%\n",
      "Epoch [3/100], Train Loss: 1.5395, Train Accuracy: 49.60%, Val Loss: 1.4343, Val Accuracy: 52.23%\n",
      "Epoch [4/100], Train Loss: 1.3401, Train Accuracy: 53.78%, Val Loss: 1.2560, Val Accuracy: 53.79%\n",
      "Epoch [5/100], Train Loss: 1.1759, Train Accuracy: 54.62%, Val Loss: 1.1186, Val Accuracy: 54.87%\n",
      "Epoch [6/100], Train Loss: 1.0599, Train Accuracy: 57.91%, Val Loss: 1.0266, Val Accuracy: 60.17%\n",
      "Epoch [7/100], Train Loss: 0.9844, Train Accuracy: 63.55%, Val Loss: 0.9664, Val Accuracy: 63.73%\n",
      "Epoch [8/100], Train Loss: 0.9320, Train Accuracy: 65.16%, Val Loss: 0.9212, Val Accuracy: 65.05%\n",
      "Epoch [9/100], Train Loss: 0.8913, Train Accuracy: 66.34%, Val Loss: 0.8842, Val Accuracy: 66.31%\n",
      "Epoch [10/100], Train Loss: 0.8581, Train Accuracy: 67.46%, Val Loss: 0.8538, Val Accuracy: 67.22%\n",
      "Epoch [11/100], Train Loss: 0.8297, Train Accuracy: 68.53%, Val Loss: 0.8269, Val Accuracy: 68.44%\n",
      "Epoch [12/100], Train Loss: 0.8050, Train Accuracy: 69.77%, Val Loss: 0.8041, Val Accuracy: 69.25%\n",
      "Epoch [13/100], Train Loss: 0.7834, Train Accuracy: 70.65%, Val Loss: 0.7839, Val Accuracy: 70.25%\n",
      "Epoch [14/100], Train Loss: 0.7645, Train Accuracy: 71.54%, Val Loss: 0.7667, Val Accuracy: 71.14%\n",
      "Epoch [15/100], Train Loss: 0.7477, Train Accuracy: 72.25%, Val Loss: 0.7509, Val Accuracy: 72.20%\n",
      "Epoch [16/100], Train Loss: 0.7326, Train Accuracy: 72.98%, Val Loss: 0.7376, Val Accuracy: 72.34%\n",
      "Epoch [17/100], Train Loss: 0.7195, Train Accuracy: 73.53%, Val Loss: 0.7245, Val Accuracy: 73.00%\n",
      "Epoch [18/100], Train Loss: 0.7074, Train Accuracy: 74.10%, Val Loss: 0.7127, Val Accuracy: 73.76%\n",
      "Epoch [19/100], Train Loss: 0.6964, Train Accuracy: 74.76%, Val Loss: 0.7028, Val Accuracy: 74.07%\n",
      "Epoch [20/100], Train Loss: 0.6861, Train Accuracy: 75.34%, Val Loss: 0.6935, Val Accuracy: 74.81%\n",
      "Epoch [21/100], Train Loss: 0.6764, Train Accuracy: 75.95%, Val Loss: 0.6854, Val Accuracy: 75.06%\n",
      "Epoch [22/100], Train Loss: 0.6679, Train Accuracy: 76.39%, Val Loss: 0.6768, Val Accuracy: 75.42%\n",
      "Epoch [23/100], Train Loss: 0.6594, Train Accuracy: 76.89%, Val Loss: 0.6685, Val Accuracy: 76.11%\n",
      "Epoch [24/100], Train Loss: 0.6516, Train Accuracy: 77.24%, Val Loss: 0.6612, Val Accuracy: 76.35%\n",
      "Epoch [25/100], Train Loss: 0.6443, Train Accuracy: 77.67%, Val Loss: 0.6538, Val Accuracy: 76.92%\n",
      "Epoch [26/100], Train Loss: 0.6374, Train Accuracy: 77.99%, Val Loss: 0.6469, Val Accuracy: 77.30%\n",
      "Epoch [27/100], Train Loss: 0.6308, Train Accuracy: 78.25%, Val Loss: 0.6398, Val Accuracy: 77.67%\n",
      "Epoch [28/100], Train Loss: 0.6246, Train Accuracy: 78.61%, Val Loss: 0.6361, Val Accuracy: 77.96%\n",
      "Epoch [29/100], Train Loss: 0.6188, Train Accuracy: 78.92%, Val Loss: 0.6296, Val Accuracy: 78.13%\n",
      "Epoch [30/100], Train Loss: 0.6131, Train Accuracy: 79.06%, Val Loss: 0.6238, Val Accuracy: 78.52%\n",
      "Epoch [31/100], Train Loss: 0.6082, Train Accuracy: 79.33%, Val Loss: 0.6184, Val Accuracy: 78.54%\n",
      "Epoch [32/100], Train Loss: 0.6030, Train Accuracy: 79.43%, Val Loss: 0.6139, Val Accuracy: 78.60%\n",
      "Epoch [33/100], Train Loss: 0.5983, Train Accuracy: 79.58%, Val Loss: 0.6097, Val Accuracy: 79.02%\n",
      "Epoch [34/100], Train Loss: 0.5940, Train Accuracy: 79.88%, Val Loss: 0.6067, Val Accuracy: 79.05%\n",
      "Epoch [35/100], Train Loss: 0.5898, Train Accuracy: 80.03%, Val Loss: 0.6013, Val Accuracy: 79.27%\n",
      "Epoch [36/100], Train Loss: 0.5857, Train Accuracy: 80.09%, Val Loss: 0.5986, Val Accuracy: 79.45%\n",
      "Epoch [37/100], Train Loss: 0.5820, Train Accuracy: 80.14%, Val Loss: 0.5943, Val Accuracy: 79.67%\n",
      "Epoch [38/100], Train Loss: 0.5785, Train Accuracy: 80.28%, Val Loss: 0.5912, Val Accuracy: 79.83%\n",
      "Epoch [39/100], Train Loss: 0.5752, Train Accuracy: 80.37%, Val Loss: 0.5878, Val Accuracy: 79.74%\n",
      "Epoch [40/100], Train Loss: 0.5719, Train Accuracy: 80.49%, Val Loss: 0.5842, Val Accuracy: 79.92%\n",
      "Epoch [41/100], Train Loss: 0.5689, Train Accuracy: 80.61%, Val Loss: 0.5817, Val Accuracy: 80.02%\n",
      "Epoch [42/100], Train Loss: 0.5659, Train Accuracy: 80.73%, Val Loss: 0.5794, Val Accuracy: 80.14%\n",
      "Epoch [43/100], Train Loss: 0.5633, Train Accuracy: 80.80%, Val Loss: 0.5782, Val Accuracy: 80.13%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _training_schedule()\n",
      "Cell \u001b[1;32mIn[14], line 27\u001b[0m, in \u001b[0;36m_training_schedule\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     model \u001b[39m=\u001b[39m TwoLayerNetwork(input_size\u001b[39m=\u001b[39minput_size, hidden_size\u001b[39m=\u001b[39mhidden_size,\n\u001b[0;32m     24\u001b[0m                             num_classes\u001b[39m=\u001b[39moutput_size, init_method\u001b[39m=\u001b[39mmethod, active_func\u001b[39m=\u001b[39mfunc)\n\u001b[0;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(hidden_size, epochs, init, active,\n\u001b[0;32m     26\u001b[0m           optimize, schedule, weight_decay, \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     train(model\u001b[39m=\u001b[39;49mmodel, opt\u001b[39m=\u001b[39;49moptm, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49mepochs, learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     28\u001b[0m           trainloader\u001b[39m=\u001b[39;49mtrainloader, valloader\u001b[39m=\u001b[39;49mvalloader, criterion\u001b[39m=\u001b[39;49mcriterion, sched\u001b[39m=\u001b[39;49mschd, weight_decay\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[0;32m     29\u001b[0m     test(model\u001b[39m=\u001b[39mmodel, device\u001b[39m=\u001b[39mdevice, testloader\u001b[39m=\u001b[39mtestloader)\n\u001b[0;32m     30\u001b[0m \u001b[39m# ✓ Learning epochs: 100, 200, 300\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, device, epochs, learning_rate, trainloader, valloader, criterion, sched, weight_decay)\u001b[0m\n\u001b[0;32m     12\u001b[0m train_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     13\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m trainloader:\n\u001b[0;32m     15\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39minput_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\VS_Workplace\\NN\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:163\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    162\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> 163\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_training_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
