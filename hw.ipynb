{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "• Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "• You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "✓ Activation function: tanh, ReLU\n",
    "\n",
    "✓ Data preprocessing\n",
    "\n",
    "✓ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "✓ Loss function: without or with the regularization term \n",
    "(L2), λ = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[𝑓(X^c, w) −y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "✓ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "✓ Learning epochs: 100, 200, 300\n",
    "\n",
    "✓ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "✓ Learning rate decay schedule: none and cosine\n",
    "\n",
    "✓ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVES = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "INIT_FUNCS = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "OPTIM_FUNCS = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr, weight_decay: optim.SGD(params=param, lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "SCHEDULERS = {\n",
    "    \"cos\": lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay:float):\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = 100. * val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'\n",
    "              .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
    "        print(\"Model Accutacy:{}\".format(val_accuracy))\n",
    "        return val_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData():\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=True, transform=transforms.transforms.ToTensor())\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=True, transform=transforms.transforms.ToTensor())\n",
    "    # Split the training set into training and validation sets\n",
    "    train_count = int(0.8 * len(trainset))\n",
    "    valid_count = len(trainset) - train_count\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset = random_split(\n",
    "        trainset, (train_count, valid_count), Generator().manual_seed(42))\n",
    "    # Create data loaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # 19\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        # 5\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            setattr(self, f\"labels_of_{col}\", d.keys())\n",
    "            reservations[col]=reservations[col].map(d.__getitem__)\n",
    "        # 17(drop id)\n",
    "        self.feature = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.booking_status = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.feature),))\n",
    "        self.classes = list(getattr(self, f\"labels_of_{reservations.columns[-1]}\"))\n",
    "    def __len__(self):\n",
    "        return len(self.booking_status)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.booking_status[idx]\n",
    "\n",
    "# kaggle: ahsan81/hotel-reservations-classification-dataset\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = HotelReservationDataset(\n",
    "        csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\")\n",
    "    class_amount = len(dataset.classes)\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    # set loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download data(zipped csv) from kaggle with username and apikey\n",
    "# import os\n",
    "# import json\n",
    "# with open(\"kaggle.json\", \"r\") as j:\n",
    "#     for (k, v) in json.load(j).items():\n",
    "#         os.environ[k] = v\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "# # https://www.kaggle.com/datasets/uciml/iris/download?datasetVersionNumber=2\n",
    "# # owner/datasetname\n",
    "# api.dataset_download_files('uciml/iris', path=\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def training_schedule():\n",
    "    sys.stdout = open(\"./result/\", \"w\")\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # ✓ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in INIT_FUNCS.items():\n",
    "                for (active, func) in ACTIVES.items():\n",
    "                    # ✓ Activation function: tanh, ReLU\n",
    "                    # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "                        # ✓ Learning rate decay schedule: none and cosine\n",
    "                        for (schedule, schd) in SCHEDULERS.items():\n",
    "                            # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "                            for weight_decay in (0.0, 0.001, 0.0001):\n",
    "                                print(hidden_size, epochs, init, active,optimize, schedule, \"start\")\n",
    "                                train(model=model, optm=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                                      trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "                                test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def _training_schedule():\n",
    "    \n",
    "    FILE_PATH = \"./{}.txt\".format(datetime.date.today())\n",
    "    counter = 1\n",
    "    test_result = {}\n",
    "\n",
    "    def write_spec_to_file():\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"- Model {} -\\n\".format(counter))\n",
    "            f.write(\"hidden nodes: {} \\nepochs: {} \\ninit: {} \\nactive: {} \\noptimize: {} \\nschedule: {} \\nweight decay: {}\\n\".format(\n",
    "                hidden_size, epochs, init, active, optimize, schedule, weight_decay))\n",
    "            f.write(\"-\"*50)\n",
    "\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    hidden_size = 5\n",
    "    epochs = 100\n",
    "    init = \"small_random\"\n",
    "    method = INIT_FUNCS[init]\n",
    "    active = \"relu\"\n",
    "    func = ACTIVES[active]\n",
    "    optimize = \"sgd\"\n",
    "    optm = OPTIM_FUNCS[optimize]\n",
    "    schedule = None\n",
    "    schd = schedule\n",
    "    weight_decay = 0.0\n",
    "    \n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        result = test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # ✓ Learning epochs: 100, 200, 300\n",
    "    for epochs in (100, 200, 300):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # Create model, optimizer, scheduler\n",
    "    for (init, method) in INIT_FUNCS.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    for (active, func) in ACTIVES.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    \n",
    "    # ✓ Activation function: tanh, ReLU\n",
    "    # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    \n",
    "    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # ✓ Learning rate decay schedule: none and cosine\n",
    "    for (schedule, schd) in SCHEDULERS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "    for weight_decay in (0.0, 0.001, 0.0001):\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "        \n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "\n",
    "    top3 = sorted(test_result, key=test_result.get, reverse=True)[:3]\n",
    "    print(\"\\nTop 3 Model:{}\\n\".format(','.join(top3)))\n",
    "    with open(FILE_PATH,\"a\") as f:\n",
    "        f.write(\"\\nTop 3 Model:{}\\n\".format(','.join(top3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "48000 12000 10000\n",
      "5 100 small_random relu sgd None 0.0 start\n",
      "Epoch [1/100], Train Loss: 2.1657, Train Accuracy: 20.84%, Val Loss: 2.0167, Val Accuracy: 26.89%\n",
      "Epoch [2/100], Train Loss: 1.8905, Train Accuracy: 30.91%, Val Loss: 1.7689, Val Accuracy: 37.03%\n",
      "Epoch [3/100], Train Loss: 1.6492, Train Accuracy: 43.42%, Val Loss: 1.5358, Val Accuracy: 49.23%\n",
      "Epoch [4/100], Train Loss: 1.4286, Train Accuracy: 53.87%, Val Loss: 1.3432, Val Accuracy: 56.38%\n",
      "Epoch [5/100], Train Loss: 1.2626, Train Accuracy: 58.99%, Val Loss: 1.2072, Val Accuracy: 59.75%\n",
      "Epoch [6/100], Train Loss: 1.1481, Train Accuracy: 61.21%, Val Loss: 1.1124, Val Accuracy: 61.08%\n",
      "Epoch [7/100], Train Loss: 1.0688, Train Accuracy: 62.51%, Val Loss: 1.0467, Val Accuracy: 62.47%\n",
      "Epoch [8/100], Train Loss: 1.0121, Train Accuracy: 63.85%, Val Loss: 0.9976, Val Accuracy: 63.43%\n",
      "Epoch [9/100], Train Loss: 0.9694, Train Accuracy: 65.06%, Val Loss: 0.9586, Val Accuracy: 64.92%\n",
      "Epoch [10/100], Train Loss: 0.9354, Train Accuracy: 66.09%, Val Loss: 0.9285, Val Accuracy: 66.42%\n",
      "Epoch [11/100], Train Loss: 0.9070, Train Accuracy: 67.11%, Val Loss: 0.9026, Val Accuracy: 67.19%\n",
      "Epoch [12/100], Train Loss: 0.8820, Train Accuracy: 68.03%, Val Loss: 0.8775, Val Accuracy: 68.64%\n",
      "Epoch [13/100], Train Loss: 0.8590, Train Accuracy: 69.07%, Val Loss: 0.8555, Val Accuracy: 69.57%\n",
      "Epoch [14/100], Train Loss: 0.8379, Train Accuracy: 70.05%, Val Loss: 0.8342, Val Accuracy: 70.29%\n",
      "Epoch [15/100], Train Loss: 0.8183, Train Accuracy: 70.88%, Val Loss: 0.8166, Val Accuracy: 70.97%\n",
      "Epoch [16/100], Train Loss: 0.8005, Train Accuracy: 71.60%, Val Loss: 0.8001, Val Accuracy: 71.49%\n",
      "Epoch [17/100], Train Loss: 0.7844, Train Accuracy: 72.14%, Val Loss: 0.7848, Val Accuracy: 71.81%\n",
      "Epoch [18/100], Train Loss: 0.7696, Train Accuracy: 72.82%, Val Loss: 0.7717, Val Accuracy: 72.33%\n",
      "Epoch [19/100], Train Loss: 0.7566, Train Accuracy: 73.25%, Val Loss: 0.7580, Val Accuracy: 73.15%\n",
      "Epoch [20/100], Train Loss: 0.7445, Train Accuracy: 73.87%, Val Loss: 0.7482, Val Accuracy: 73.39%\n",
      "Epoch [21/100], Train Loss: 0.7336, Train Accuracy: 74.30%, Val Loss: 0.7364, Val Accuracy: 74.07%\n",
      "Epoch [22/100], Train Loss: 0.7239, Train Accuracy: 74.84%, Val Loss: 0.7290, Val Accuracy: 74.19%\n",
      "Epoch [23/100], Train Loss: 0.7147, Train Accuracy: 75.24%, Val Loss: 0.7193, Val Accuracy: 74.83%\n",
      "Epoch [24/100], Train Loss: 0.7062, Train Accuracy: 75.66%, Val Loss: 0.7114, Val Accuracy: 75.19%\n",
      "Epoch [25/100], Train Loss: 0.6982, Train Accuracy: 76.00%, Val Loss: 0.7049, Val Accuracy: 75.27%\n",
      "Epoch [26/100], Train Loss: 0.6909, Train Accuracy: 76.34%, Val Loss: 0.6986, Val Accuracy: 75.51%\n",
      "Epoch [27/100], Train Loss: 0.6839, Train Accuracy: 76.63%, Val Loss: 0.6922, Val Accuracy: 76.13%\n",
      "Epoch [28/100], Train Loss: 0.6774, Train Accuracy: 76.96%, Val Loss: 0.6841, Val Accuracy: 76.31%\n",
      "Epoch [29/100], Train Loss: 0.6709, Train Accuracy: 77.27%, Val Loss: 0.6784, Val Accuracy: 76.67%\n",
      "Epoch [30/100], Train Loss: 0.6651, Train Accuracy: 77.49%, Val Loss: 0.6730, Val Accuracy: 76.69%\n",
      "Epoch [31/100], Train Loss: 0.6595, Train Accuracy: 77.62%, Val Loss: 0.6674, Val Accuracy: 76.89%\n",
      "Epoch [32/100], Train Loss: 0.6541, Train Accuracy: 77.94%, Val Loss: 0.6622, Val Accuracy: 77.12%\n",
      "Epoch [33/100], Train Loss: 0.6490, Train Accuracy: 78.20%, Val Loss: 0.6585, Val Accuracy: 77.50%\n",
      "Epoch [34/100], Train Loss: 0.6443, Train Accuracy: 78.30%, Val Loss: 0.6529, Val Accuracy: 77.69%\n",
      "Epoch [35/100], Train Loss: 0.6396, Train Accuracy: 78.49%, Val Loss: 0.6495, Val Accuracy: 77.69%\n",
      "Epoch [36/100], Train Loss: 0.6350, Train Accuracy: 78.55%, Val Loss: 0.6448, Val Accuracy: 77.79%\n",
      "Epoch [37/100], Train Loss: 0.6307, Train Accuracy: 78.82%, Val Loss: 0.6414, Val Accuracy: 77.95%\n",
      "Epoch [38/100], Train Loss: 0.6266, Train Accuracy: 78.94%, Val Loss: 0.6367, Val Accuracy: 78.38%\n",
      "Epoch [39/100], Train Loss: 0.6228, Train Accuracy: 79.06%, Val Loss: 0.6329, Val Accuracy: 78.47%\n",
      "Epoch [40/100], Train Loss: 0.6190, Train Accuracy: 79.13%, Val Loss: 0.6299, Val Accuracy: 78.52%\n",
      "Epoch [41/100], Train Loss: 0.6153, Train Accuracy: 79.30%, Val Loss: 0.6260, Val Accuracy: 78.57%\n",
      "Epoch [42/100], Train Loss: 0.6119, Train Accuracy: 79.31%, Val Loss: 0.6225, Val Accuracy: 78.67%\n",
      "Epoch [43/100], Train Loss: 0.6085, Train Accuracy: 79.45%, Val Loss: 0.6201, Val Accuracy: 78.75%\n",
      "Epoch [44/100], Train Loss: 0.6052, Train Accuracy: 79.54%, Val Loss: 0.6173, Val Accuracy: 78.83%\n",
      "Epoch [45/100], Train Loss: 0.6022, Train Accuracy: 79.63%, Val Loss: 0.6134, Val Accuracy: 79.03%\n",
      "Epoch [46/100], Train Loss: 0.5990, Train Accuracy: 79.77%, Val Loss: 0.6117, Val Accuracy: 79.03%\n",
      "Epoch [47/100], Train Loss: 0.5962, Train Accuracy: 79.83%, Val Loss: 0.6082, Val Accuracy: 79.17%\n",
      "Epoch [48/100], Train Loss: 0.5935, Train Accuracy: 79.91%, Val Loss: 0.6052, Val Accuracy: 79.30%\n",
      "Epoch [49/100], Train Loss: 0.5907, Train Accuracy: 79.96%, Val Loss: 0.6031, Val Accuracy: 79.25%\n",
      "Epoch [50/100], Train Loss: 0.5881, Train Accuracy: 80.09%, Val Loss: 0.6017, Val Accuracy: 79.51%\n",
      "Epoch [51/100], Train Loss: 0.5855, Train Accuracy: 80.21%, Val Loss: 0.5988, Val Accuracy: 79.50%\n",
      "Epoch [52/100], Train Loss: 0.5831, Train Accuracy: 80.23%, Val Loss: 0.5971, Val Accuracy: 79.59%\n",
      "Epoch [53/100], Train Loss: 0.5805, Train Accuracy: 80.40%, Val Loss: 0.5937, Val Accuracy: 79.85%\n",
      "Epoch [54/100], Train Loss: 0.5784, Train Accuracy: 80.38%, Val Loss: 0.5914, Val Accuracy: 79.88%\n",
      "Epoch [55/100], Train Loss: 0.5762, Train Accuracy: 80.45%, Val Loss: 0.5909, Val Accuracy: 79.90%\n",
      "Epoch [56/100], Train Loss: 0.5736, Train Accuracy: 80.56%, Val Loss: 0.5892, Val Accuracy: 79.97%\n",
      "Epoch [57/100], Train Loss: 0.5716, Train Accuracy: 80.56%, Val Loss: 0.5868, Val Accuracy: 79.90%\n",
      "Epoch [58/100], Train Loss: 0.5697, Train Accuracy: 80.67%, Val Loss: 0.5842, Val Accuracy: 80.02%\n",
      "Epoch [59/100], Train Loss: 0.5677, Train Accuracy: 80.73%, Val Loss: 0.5825, Val Accuracy: 80.12%\n",
      "Epoch [60/100], Train Loss: 0.5657, Train Accuracy: 80.78%, Val Loss: 0.5807, Val Accuracy: 80.20%\n",
      "Epoch [61/100], Train Loss: 0.5638, Train Accuracy: 80.87%, Val Loss: 0.5802, Val Accuracy: 80.28%\n",
      "Epoch [62/100], Train Loss: 0.5619, Train Accuracy: 80.85%, Val Loss: 0.5783, Val Accuracy: 80.14%\n",
      "Epoch [63/100], Train Loss: 0.5602, Train Accuracy: 80.94%, Val Loss: 0.5749, Val Accuracy: 80.35%\n",
      "Epoch [64/100], Train Loss: 0.5582, Train Accuracy: 81.05%, Val Loss: 0.5751, Val Accuracy: 80.27%\n",
      "Epoch [65/100], Train Loss: 0.5567, Train Accuracy: 81.06%, Val Loss: 0.5730, Val Accuracy: 80.34%\n",
      "Epoch [66/100], Train Loss: 0.5548, Train Accuracy: 81.12%, Val Loss: 0.5730, Val Accuracy: 80.35%\n",
      "Epoch [67/100], Train Loss: 0.5534, Train Accuracy: 81.14%, Val Loss: 0.5696, Val Accuracy: 80.52%\n",
      "Epoch [68/100], Train Loss: 0.5517, Train Accuracy: 81.24%, Val Loss: 0.5682, Val Accuracy: 80.71%\n",
      "Epoch [69/100], Train Loss: 0.5503, Train Accuracy: 81.29%, Val Loss: 0.5669, Val Accuracy: 80.60%\n",
      "Epoch [70/100], Train Loss: 0.5486, Train Accuracy: 81.36%, Val Loss: 0.5664, Val Accuracy: 80.58%\n",
      "Epoch [71/100], Train Loss: 0.5472, Train Accuracy: 81.37%, Val Loss: 0.5640, Val Accuracy: 80.68%\n",
      "Epoch [72/100], Train Loss: 0.5459, Train Accuracy: 81.40%, Val Loss: 0.5632, Val Accuracy: 80.80%\n",
      "Epoch [73/100], Train Loss: 0.5446, Train Accuracy: 81.46%, Val Loss: 0.5618, Val Accuracy: 80.89%\n",
      "Epoch [74/100], Train Loss: 0.5433, Train Accuracy: 81.47%, Val Loss: 0.5606, Val Accuracy: 80.86%\n",
      "Epoch [75/100], Train Loss: 0.5417, Train Accuracy: 81.61%, Val Loss: 0.5594, Val Accuracy: 80.83%\n",
      "Epoch [76/100], Train Loss: 0.5404, Train Accuracy: 81.61%, Val Loss: 0.5593, Val Accuracy: 80.78%\n",
      "Epoch [77/100], Train Loss: 0.5393, Train Accuracy: 81.62%, Val Loss: 0.5562, Val Accuracy: 81.07%\n",
      "Epoch [78/100], Train Loss: 0.5381, Train Accuracy: 81.65%, Val Loss: 0.5553, Val Accuracy: 81.08%\n",
      "Epoch [79/100], Train Loss: 0.5367, Train Accuracy: 81.64%, Val Loss: 0.5547, Val Accuracy: 81.13%\n",
      "Epoch [80/100], Train Loss: 0.5355, Train Accuracy: 81.81%, Val Loss: 0.5540, Val Accuracy: 81.03%\n",
      "Epoch [81/100], Train Loss: 0.5344, Train Accuracy: 81.81%, Val Loss: 0.5522, Val Accuracy: 81.20%\n",
      "Epoch [82/100], Train Loss: 0.5334, Train Accuracy: 81.77%, Val Loss: 0.5532, Val Accuracy: 81.08%\n",
      "Epoch [83/100], Train Loss: 0.5323, Train Accuracy: 81.85%, Val Loss: 0.5513, Val Accuracy: 81.22%\n",
      "Epoch [84/100], Train Loss: 0.5311, Train Accuracy: 81.90%, Val Loss: 0.5497, Val Accuracy: 81.33%\n",
      "Epoch [85/100], Train Loss: 0.5300, Train Accuracy: 81.97%, Val Loss: 0.5493, Val Accuracy: 81.30%\n",
      "Epoch [86/100], Train Loss: 0.5289, Train Accuracy: 81.90%, Val Loss: 0.5477, Val Accuracy: 81.33%\n",
      "Epoch [87/100], Train Loss: 0.5280, Train Accuracy: 81.96%, Val Loss: 0.5479, Val Accuracy: 81.47%\n",
      "Epoch [88/100], Train Loss: 0.5270, Train Accuracy: 82.01%, Val Loss: 0.5458, Val Accuracy: 81.32%\n",
      "Epoch [89/100], Train Loss: 0.5259, Train Accuracy: 82.06%, Val Loss: 0.5468, Val Accuracy: 81.38%\n",
      "Epoch [90/100], Train Loss: 0.5251, Train Accuracy: 82.10%, Val Loss: 0.5447, Val Accuracy: 81.53%\n",
      "Epoch [91/100], Train Loss: 0.5241, Train Accuracy: 82.21%, Val Loss: 0.5437, Val Accuracy: 81.59%\n",
      "Epoch [92/100], Train Loss: 0.5231, Train Accuracy: 82.20%, Val Loss: 0.5427, Val Accuracy: 81.52%\n",
      "Epoch [93/100], Train Loss: 0.5223, Train Accuracy: 82.24%, Val Loss: 0.5417, Val Accuracy: 81.71%\n",
      "Epoch [94/100], Train Loss: 0.5215, Train Accuracy: 82.15%, Val Loss: 0.5407, Val Accuracy: 81.64%\n",
      "Epoch [95/100], Train Loss: 0.5205, Train Accuracy: 82.26%, Val Loss: 0.5417, Val Accuracy: 81.55%\n",
      "Epoch [96/100], Train Loss: 0.5196, Train Accuracy: 82.22%, Val Loss: 0.5390, Val Accuracy: 81.85%\n",
      "Epoch [97/100], Train Loss: 0.5188, Train Accuracy: 82.33%, Val Loss: 0.5389, Val Accuracy: 81.78%\n",
      "Epoch [98/100], Train Loss: 0.5180, Train Accuracy: 82.30%, Val Loss: 0.5375, Val Accuracy: 81.86%\n",
      "Epoch [99/100], Train Loss: 0.5170, Train Accuracy: 82.26%, Val Loss: 0.5366, Val Accuracy: 81.75%\n",
      "Epoch [100/100], Train Loss: 0.5162, Train Accuracy: 82.43%, Val Loss: 0.5371, Val Accuracy: 81.86%\n",
      "Model Accutacy:81.18\n",
      "8 100 small_random relu sgd None 0.0 start\n",
      "Epoch [1/100], Train Loss: 2.2446, Train Accuracy: 12.07%, Val Loss: 2.1368, Val Accuracy: 9.09%\n",
      "Epoch [2/100], Train Loss: 1.9918, Train Accuracy: 16.87%, Val Loss: 1.8213, Val Accuracy: 32.57%\n",
      "Epoch [3/100], Train Loss: 1.6050, Train Accuracy: 46.99%, Val Loss: 1.4141, Val Accuracy: 56.26%\n",
      "Epoch [4/100], Train Loss: 1.2611, Train Accuracy: 60.45%, Val Loss: 1.1566, Val Accuracy: 62.49%\n",
      "Epoch [5/100], Train Loss: 1.0665, Train Accuracy: 65.16%, Val Loss: 1.0127, Val Accuracy: 65.88%\n",
      "Epoch [6/100], Train Loss: 0.9496, Train Accuracy: 67.58%, Val Loss: 0.9189, Val Accuracy: 67.98%\n",
      "Epoch [7/100], Train Loss: 0.8719, Train Accuracy: 69.47%, Val Loss: 0.8563, Val Accuracy: 69.25%\n",
      "Epoch [8/100], Train Loss: 0.8184, Train Accuracy: 71.21%, Val Loss: 0.8116, Val Accuracy: 70.97%\n",
      "Epoch [9/100], Train Loss: 0.7782, Train Accuracy: 72.71%, Val Loss: 0.7760, Val Accuracy: 72.37%\n",
      "Epoch [10/100], Train Loss: 0.7458, Train Accuracy: 73.99%, Val Loss: 0.7479, Val Accuracy: 73.58%\n",
      "Epoch [11/100], Train Loss: 0.7186, Train Accuracy: 75.30%, Val Loss: 0.7224, Val Accuracy: 74.83%\n",
      "Epoch [12/100], Train Loss: 0.6954, Train Accuracy: 76.15%, Val Loss: 0.7007, Val Accuracy: 75.49%\n",
      "Epoch [13/100], Train Loss: 0.6752, Train Accuracy: 76.89%, Val Loss: 0.6817, Val Accuracy: 76.31%\n",
      "Epoch [14/100], Train Loss: 0.6577, Train Accuracy: 77.67%, Val Loss: 0.6650, Val Accuracy: 76.78%\n",
      "Epoch [15/100], Train Loss: 0.6422, Train Accuracy: 78.23%, Val Loss: 0.6507, Val Accuracy: 77.27%\n",
      "Epoch [16/100], Train Loss: 0.6283, Train Accuracy: 78.75%, Val Loss: 0.6374, Val Accuracy: 77.72%\n",
      "Epoch [17/100], Train Loss: 0.6161, Train Accuracy: 79.01%, Val Loss: 0.6256, Val Accuracy: 78.35%\n",
      "Epoch [18/100], Train Loss: 0.6053, Train Accuracy: 79.53%, Val Loss: 0.6157, Val Accuracy: 78.82%\n",
      "Epoch [19/100], Train Loss: 0.5955, Train Accuracy: 79.76%, Val Loss: 0.6078, Val Accuracy: 79.07%\n",
      "Epoch [20/100], Train Loss: 0.5868, Train Accuracy: 80.05%, Val Loss: 0.5983, Val Accuracy: 79.30%\n",
      "Epoch [21/100], Train Loss: 0.5788, Train Accuracy: 80.31%, Val Loss: 0.5909, Val Accuracy: 79.52%\n",
      "Epoch [22/100], Train Loss: 0.5715, Train Accuracy: 80.57%, Val Loss: 0.5837, Val Accuracy: 79.88%\n",
      "Epoch [23/100], Train Loss: 0.5653, Train Accuracy: 80.71%, Val Loss: 0.5778, Val Accuracy: 80.28%\n",
      "Epoch [24/100], Train Loss: 0.5592, Train Accuracy: 80.92%, Val Loss: 0.5719, Val Accuracy: 80.28%\n",
      "Epoch [25/100], Train Loss: 0.5534, Train Accuracy: 81.10%, Val Loss: 0.5668, Val Accuracy: 80.57%\n",
      "Epoch [26/100], Train Loss: 0.5483, Train Accuracy: 81.27%, Val Loss: 0.5622, Val Accuracy: 80.74%\n",
      "Epoch [27/100], Train Loss: 0.5436, Train Accuracy: 81.44%, Val Loss: 0.5581, Val Accuracy: 80.71%\n",
      "Epoch [28/100], Train Loss: 0.5388, Train Accuracy: 81.52%, Val Loss: 0.5544, Val Accuracy: 81.01%\n",
      "Epoch [29/100], Train Loss: 0.5348, Train Accuracy: 81.69%, Val Loss: 0.5500, Val Accuracy: 80.98%\n",
      "Epoch [30/100], Train Loss: 0.5309, Train Accuracy: 81.83%, Val Loss: 0.5454, Val Accuracy: 81.27%\n",
      "Epoch [31/100], Train Loss: 0.5271, Train Accuracy: 81.95%, Val Loss: 0.5424, Val Accuracy: 81.32%\n",
      "Epoch [32/100], Train Loss: 0.5235, Train Accuracy: 81.98%, Val Loss: 0.5402, Val Accuracy: 81.20%\n",
      "Epoch [33/100], Train Loss: 0.5203, Train Accuracy: 82.13%, Val Loss: 0.5355, Val Accuracy: 81.47%\n",
      "Epoch [34/100], Train Loss: 0.5171, Train Accuracy: 82.20%, Val Loss: 0.5335, Val Accuracy: 81.58%\n",
      "Epoch [35/100], Train Loss: 0.5140, Train Accuracy: 82.27%, Val Loss: 0.5293, Val Accuracy: 81.72%\n",
      "Epoch [36/100], Train Loss: 0.5110, Train Accuracy: 82.35%, Val Loss: 0.5286, Val Accuracy: 81.83%\n",
      "Epoch [37/100], Train Loss: 0.5084, Train Accuracy: 82.48%, Val Loss: 0.5267, Val Accuracy: 81.86%\n",
      "Epoch [38/100], Train Loss: 0.5059, Train Accuracy: 82.55%, Val Loss: 0.5223, Val Accuracy: 81.88%\n",
      "Epoch [39/100], Train Loss: 0.5033, Train Accuracy: 82.59%, Val Loss: 0.5219, Val Accuracy: 82.06%\n",
      "Epoch [40/100], Train Loss: 0.5010, Train Accuracy: 82.70%, Val Loss: 0.5180, Val Accuracy: 82.03%\n",
      "Epoch [41/100], Train Loss: 0.4986, Train Accuracy: 82.71%, Val Loss: 0.5179, Val Accuracy: 82.24%\n",
      "Epoch [42/100], Train Loss: 0.4965, Train Accuracy: 82.74%, Val Loss: 0.5148, Val Accuracy: 82.17%\n",
      "Epoch [43/100], Train Loss: 0.4945, Train Accuracy: 82.83%, Val Loss: 0.5121, Val Accuracy: 82.16%\n",
      "Epoch [44/100], Train Loss: 0.4925, Train Accuracy: 82.90%, Val Loss: 0.5096, Val Accuracy: 82.40%\n",
      "Epoch [45/100], Train Loss: 0.4906, Train Accuracy: 83.00%, Val Loss: 0.5089, Val Accuracy: 82.25%\n",
      "Epoch [46/100], Train Loss: 0.4884, Train Accuracy: 83.03%, Val Loss: 0.5079, Val Accuracy: 82.31%\n",
      "Epoch [47/100], Train Loss: 0.4869, Train Accuracy: 83.12%, Val Loss: 0.5056, Val Accuracy: 82.53%\n",
      "Epoch [48/100], Train Loss: 0.4851, Train Accuracy: 83.17%, Val Loss: 0.5026, Val Accuracy: 82.57%\n",
      "Epoch [49/100], Train Loss: 0.4835, Train Accuracy: 83.22%, Val Loss: 0.5019, Val Accuracy: 82.50%\n",
      "Epoch [50/100], Train Loss: 0.4818, Train Accuracy: 83.24%, Val Loss: 0.4994, Val Accuracy: 82.70%\n",
      "Epoch [51/100], Train Loss: 0.4801, Train Accuracy: 83.43%, Val Loss: 0.4980, Val Accuracy: 82.79%\n",
      "Epoch [52/100], Train Loss: 0.4788, Train Accuracy: 83.39%, Val Loss: 0.4975, Val Accuracy: 82.55%\n",
      "Epoch [53/100], Train Loss: 0.4774, Train Accuracy: 83.53%, Val Loss: 0.4959, Val Accuracy: 82.64%\n",
      "Epoch [54/100], Train Loss: 0.4758, Train Accuracy: 83.53%, Val Loss: 0.4950, Val Accuracy: 82.92%\n",
      "Epoch [55/100], Train Loss: 0.4747, Train Accuracy: 83.60%, Val Loss: 0.4938, Val Accuracy: 82.81%\n",
      "Epoch [56/100], Train Loss: 0.4733, Train Accuracy: 83.59%, Val Loss: 0.4929, Val Accuracy: 82.70%\n",
      "Epoch [57/100], Train Loss: 0.4719, Train Accuracy: 83.67%, Val Loss: 0.4932, Val Accuracy: 83.03%\n",
      "Epoch [58/100], Train Loss: 0.4707, Train Accuracy: 83.74%, Val Loss: 0.4904, Val Accuracy: 82.78%\n",
      "Epoch [59/100], Train Loss: 0.4695, Train Accuracy: 83.75%, Val Loss: 0.4888, Val Accuracy: 82.88%\n",
      "Epoch [60/100], Train Loss: 0.4685, Train Accuracy: 83.79%, Val Loss: 0.4888, Val Accuracy: 82.87%\n",
      "Epoch [61/100], Train Loss: 0.4671, Train Accuracy: 83.83%, Val Loss: 0.4867, Val Accuracy: 82.99%\n",
      "Epoch [62/100], Train Loss: 0.4661, Train Accuracy: 83.89%, Val Loss: 0.4851, Val Accuracy: 83.09%\n",
      "Epoch [63/100], Train Loss: 0.4647, Train Accuracy: 83.91%, Val Loss: 0.4853, Val Accuracy: 83.23%\n",
      "Epoch [64/100], Train Loss: 0.4640, Train Accuracy: 83.95%, Val Loss: 0.4840, Val Accuracy: 83.03%\n",
      "Epoch [65/100], Train Loss: 0.4627, Train Accuracy: 83.99%, Val Loss: 0.4825, Val Accuracy: 83.22%\n",
      "Epoch [66/100], Train Loss: 0.4618, Train Accuracy: 84.08%, Val Loss: 0.4812, Val Accuracy: 83.23%\n",
      "Epoch [67/100], Train Loss: 0.4608, Train Accuracy: 84.09%, Val Loss: 0.4803, Val Accuracy: 83.29%\n",
      "Epoch [68/100], Train Loss: 0.4598, Train Accuracy: 84.09%, Val Loss: 0.4791, Val Accuracy: 83.41%\n",
      "Epoch [69/100], Train Loss: 0.4588, Train Accuracy: 84.12%, Val Loss: 0.4786, Val Accuracy: 83.40%\n",
      "Epoch [70/100], Train Loss: 0.4579, Train Accuracy: 84.14%, Val Loss: 0.4776, Val Accuracy: 83.41%\n",
      "Epoch [71/100], Train Loss: 0.4569, Train Accuracy: 84.20%, Val Loss: 0.4773, Val Accuracy: 83.47%\n",
      "Epoch [72/100], Train Loss: 0.4560, Train Accuracy: 84.24%, Val Loss: 0.4779, Val Accuracy: 83.46%\n",
      "Epoch [73/100], Train Loss: 0.4553, Train Accuracy: 84.25%, Val Loss: 0.4751, Val Accuracy: 83.46%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _training_schedule()\n",
      "Cell \u001b[1;32mIn[23], line 44\u001b[0m, in \u001b[0;36m_training_schedule\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m write_spec_to_file()\n\u001b[1;32m---> 44\u001b[0m train(model\u001b[39m=\u001b[39;49mmodel, opt\u001b[39m=\u001b[39;49moptm, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49mepochs, learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     45\u001b[0m       trainloader\u001b[39m=\u001b[39;49mtrainloader, valloader\u001b[39m=\u001b[39;49mvalloader, criterion\u001b[39m=\u001b[39;49mcriterion, sched\u001b[39m=\u001b[39;49mschd, weight_decay\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[0;32m     46\u001b[0m result \u001b[39m=\u001b[39m test(model\u001b[39m=\u001b[39mmodel, device\u001b[39m=\u001b[39mdevice, testloader\u001b[39m=\u001b[39mtestloader)\n\u001b[0;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(FILE_PATH,\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, device, epochs, learning_rate, trainloader, valloader, criterion, sched, weight_decay)\u001b[0m\n\u001b[0;32m     31\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     32\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m valloader:\n\u001b[0;32m     34\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39minput_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_training_schedule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
