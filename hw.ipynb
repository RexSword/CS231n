{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â€¢ Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "â€¢ Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "â€¢ You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "âœ“ Activation function: tanh, ReLU\n",
    "\n",
    "âœ“ Data preprocessing\n",
    "\n",
    "âœ“ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "âœ“ Loss function: without or with the regularization term \n",
    "(L2), Î» = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[ð‘“(X^c, w) âˆ’y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "âœ“ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "âœ“ Learning epochs: 100, 200, 300\n",
    "\n",
    "âœ“ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "âœ“ Learning rate decay schedule: none and cosine\n",
    "\n",
    "âœ“ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Type\n",
    "from operator import mul\n",
    "\n",
    "def product(nums: Iterable[Type], func: Callable[[Type, Type], Type] = mul):\n",
    "    def _product(nums):\n",
    "        nonlocal func\n",
    "        if len(nums) == 1:\n",
    "            return nums[0]\n",
    "        return func(nums[-1], _product(nums[:-1]))\n",
    "    try:\n",
    "        return _product(nums)\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVES = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "INIT_FUNCS = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "OPTIM_FUNCS = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr, weight_decay: optim.SGD(params=param, lr=lr, momentum=0.9, weight_decay=weight_decay),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "SCHEDULERS = {\n",
    "    \"cos\": lambda opt: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler, weight_decay:float):\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = 100. * val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'\n",
    "              .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
    "        print(\"Model Accutacy:{}\".format(val_accuracy))\n",
    "        return val_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def getPytorchData():\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=True, download=True, transform=transforms.transforms.ToTensor())\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    class_amount = len(trainset.classes)\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root=\"./data/\", train=False, download=True, transform=transforms.transforms.ToTensor())\n",
    "    # Split the training set into training and validation sets\n",
    "    train_count = int(0.8 * len(trainset))\n",
    "    valid_count = len(trainset) - train_count\n",
    "    print(train_count, valid_count, len(testset))\n",
    "    trainset, valset = random_split(\n",
    "        trainset, (train_count, valid_count), Generator().manual_seed(42))\n",
    "    # Create data loaders to load the data in batches\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # 19\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        # 5\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            setattr(self, f\"labels_of_{col}\", d.keys())\n",
    "            reservations[col]=reservations[col].map(d.__getitem__)\n",
    "        # 17(drop id)\n",
    "        self.feature = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        # two status\n",
    "        self.booking_status = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.feature),))\n",
    "        self.classes = list(getattr(self, f\"labels_of_{reservations.columns[-1]}\"))\n",
    "    def __len__(self):\n",
    "        return len(self.booking_status)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.feature[idx], self.booking_status[idx]\n",
    "\n",
    "# kaggle: ahsan81/hotel-reservations-classification-dataset\n",
    "def getCustomizedData():\n",
    "    # preprocess\n",
    "    dataset = HotelReservationDataset(\n",
    "        csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\")\n",
    "    class_amount = len(dataset.classes)\n",
    "    # train test split\n",
    "    train_count = int(0.7 * len(dataset))\n",
    "    valid_count = int(0.2 * len(dataset))\n",
    "    test_count = len(dataset) - train_count - valid_count\n",
    "    print(train_count, valid_count, test_count)\n",
    "    trainset, valset, testset = random_split(\n",
    "        dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "    datum_size = product(trainset[0][0].size())\n",
    "    # set loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=True)\n",
    "    return trainloader, valloader, testloader, datum_size, class_amount\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download data(zipped csv) from kaggle with username and apikey\n",
    "# import os\n",
    "# import json\n",
    "# with open(\"kaggle.json\", \"r\") as j:\n",
    "#     for (k, v) in json.load(j).items():\n",
    "#         os.environ[k] = v\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "# # https://www.kaggle.com/datasets/uciml/iris/download?datasetVersionNumber=2\n",
    "# # owner/datasetname\n",
    "# api.dataset_download_files('uciml/iris', path=\"./data/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def training_schedule():\n",
    "    sys.stdout = open(\"./result/\", \"w\")\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # âœ“ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # âœ“ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in INIT_FUNCS.items():\n",
    "                for (active, func) in ACTIVES.items():\n",
    "                    # âœ“ Activation function: tanh, ReLU\n",
    "                    # âœ“ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # âœ“ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "                        # âœ“ Learning rate decay schedule: none and cosine\n",
    "                        for (schedule, schd) in SCHEDULERS.items():\n",
    "                            # âœ“ Loss function: without or with L2, Î» = 0.001 or 0.0001\n",
    "                            for weight_decay in (0.0, 0.001, 0.0001):\n",
    "                                print(hidden_size, epochs, init, active,optimize, schedule, \"start\")\n",
    "                                train(model=model, optm=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                                      trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "                                test(model=model, device=device, testloader=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def _training_schedule():\n",
    "    \n",
    "    FILE_PATH = \"./{}.txt\".format(datetime.date.today())\n",
    "    counter = 1\n",
    "    test_result = {}\n",
    "\n",
    "    def write_spec_to_file():\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"- Model {} -\\n\".format(counter))\n",
    "            f.write(\"hidden nodes: {} \\nepochs: {} \\ninit: {} \\nactive: {} \\noptimize: {} \\nschedule: {} \\nweight decay: {}\\n\".format(\n",
    "                hidden_size, epochs, init, active, optimize, schedule, weight_decay))\n",
    "            f.write(\"-\"*50)\n",
    "\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    # hyper parameters\n",
    "    trainloader, valloader, testloader, input_size, output_size = getPytorchData()\n",
    "    learning_rate = 0.001\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    hidden_size = 5\n",
    "    epochs = 100\n",
    "    init = \"small_random\"\n",
    "    method = INIT_FUNCS[init]\n",
    "    active = \"relu\"\n",
    "    func = ACTIVES[active]\n",
    "    optimize = \"sgd\"\n",
    "    optm = OPTIM_FUNCS[optimize]\n",
    "    schedule = None\n",
    "    schd = schedule\n",
    "    weight_decay = 0.0\n",
    "    \n",
    "    # âœ“ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        result = test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # âœ“ Learning epochs: 100, 200, 300\n",
    "    for epochs in (100, 200, 300):\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # Create model, optimizer, scheduler\n",
    "    for (init, method) in INIT_FUNCS.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    for (active, func) in ACTIVES.items():\n",
    "        model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                                num_classes=output_size, init_method=method, active_func=func)\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    \n",
    "    # âœ“ Activation function: tanh, ReLU\n",
    "    # âœ“ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "    model = TwoLayerNetwork(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_classes=output_size, init_method=method, active_func=func)\n",
    "    \n",
    "    # âœ“ Optimizer: gradient descent, Momentum, Adam\n",
    "    for (optimize, optm) in OPTIM_FUNCS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # âœ“ Learning rate decay schedule: none and cosine\n",
    "    for (schedule, schd) in SCHEDULERS.items():\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "\n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "    # âœ“ Loss function: without or with L2, Î» = 0.001 or 0.0001\n",
    "    for weight_decay in (0.0, 0.001, 0.0001):\n",
    "        print(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \"start\")\n",
    "        write_spec_to_file()\n",
    "        \n",
    "        train(model=model, opt=optm, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "              trainloader=trainloader, valloader=valloader, criterion=criterion, sched=schd, weight_decay=weight_decay)\n",
    "        test(model=model, device=device, testloader=testloader)\n",
    "        with open(FILE_PATH,\"a\") as f:\n",
    "            f.write(\"\\nModel Accuracy:{}\\n\".format(result))\n",
    "            f.write(\"-\"*50+\"\\n\")\n",
    "        test_result[\"model{}\".format(counter)] = result\n",
    "        counter += 1\n",
    "\n",
    "    top3 = sorted(test_result, key=test_result.get, reverse=True)[:3]\n",
    "    print(\"\\nTop 3 Model:{}\\n\".format(','.join(top3)))\n",
    "    with open(FILE_PATH,\"a\") as f:\n",
    "        f.write(\"\\nTop 3 Model:{}\\n\".format(','.join(top3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\raw\n",
      "\n",
      "48000 12000 10000\n",
      "5 100 small_random relu sgd None 0.0 start\n",
      "Epoch [1/100], Train Loss: 2.1657, Train Accuracy: 20.84%, Val Loss: 2.0167, Val Accuracy: 26.89%\n",
      "Epoch [2/100], Train Loss: 1.8905, Train Accuracy: 30.91%, Val Loss: 1.7689, Val Accuracy: 37.03%\n",
      "Epoch [3/100], Train Loss: 1.6492, Train Accuracy: 43.42%, Val Loss: 1.5358, Val Accuracy: 49.23%\n",
      "Epoch [4/100], Train Loss: 1.4286, Train Accuracy: 53.87%, Val Loss: 1.3432, Val Accuracy: 56.38%\n",
      "Epoch [5/100], Train Loss: 1.2626, Train Accuracy: 58.99%, Val Loss: 1.2072, Val Accuracy: 59.75%\n",
      "Epoch [6/100], Train Loss: 1.1481, Train Accuracy: 61.21%, Val Loss: 1.1124, Val Accuracy: 61.08%\n",
      "Epoch [7/100], Train Loss: 1.0688, Train Accuracy: 62.51%, Val Loss: 1.0467, Val Accuracy: 62.47%\n",
      "Epoch [8/100], Train Loss: 1.0121, Train Accuracy: 63.85%, Val Loss: 0.9976, Val Accuracy: 63.43%\n",
      "Epoch [9/100], Train Loss: 0.9694, Train Accuracy: 65.06%, Val Loss: 0.9586, Val Accuracy: 64.92%\n",
      "Epoch [10/100], Train Loss: 0.9354, Train Accuracy: 66.09%, Val Loss: 0.9285, Val Accuracy: 66.42%\n",
      "Epoch [11/100], Train Loss: 0.9070, Train Accuracy: 67.11%, Val Loss: 0.9026, Val Accuracy: 67.19%\n",
      "Epoch [12/100], Train Loss: 0.8820, Train Accuracy: 68.03%, Val Loss: 0.8775, Val Accuracy: 68.64%\n",
      "Epoch [13/100], Train Loss: 0.8590, Train Accuracy: 69.07%, Val Loss: 0.8555, Val Accuracy: 69.57%\n",
      "Epoch [14/100], Train Loss: 0.8379, Train Accuracy: 70.05%, Val Loss: 0.8342, Val Accuracy: 70.29%\n",
      "Epoch [15/100], Train Loss: 0.8183, Train Accuracy: 70.88%, Val Loss: 0.8166, Val Accuracy: 70.97%\n",
      "Epoch [16/100], Train Loss: 0.8005, Train Accuracy: 71.60%, Val Loss: 0.8001, Val Accuracy: 71.49%\n",
      "Epoch [17/100], Train Loss: 0.7844, Train Accuracy: 72.14%, Val Loss: 0.7848, Val Accuracy: 71.81%\n",
      "Epoch [18/100], Train Loss: 0.7696, Train Accuracy: 72.82%, Val Loss: 0.7717, Val Accuracy: 72.33%\n",
      "Epoch [19/100], Train Loss: 0.7566, Train Accuracy: 73.25%, Val Loss: 0.7580, Val Accuracy: 73.15%\n",
      "Epoch [20/100], Train Loss: 0.7445, Train Accuracy: 73.87%, Val Loss: 0.7482, Val Accuracy: 73.39%\n",
      "Epoch [21/100], Train Loss: 0.7336, Train Accuracy: 74.30%, Val Loss: 0.7364, Val Accuracy: 74.07%\n",
      "Epoch [22/100], Train Loss: 0.7239, Train Accuracy: 74.84%, Val Loss: 0.7290, Val Accuracy: 74.19%\n",
      "Epoch [23/100], Train Loss: 0.7147, Train Accuracy: 75.24%, Val Loss: 0.7193, Val Accuracy: 74.83%\n",
      "Epoch [24/100], Train Loss: 0.7062, Train Accuracy: 75.66%, Val Loss: 0.7114, Val Accuracy: 75.19%\n",
      "Epoch [25/100], Train Loss: 0.6982, Train Accuracy: 76.00%, Val Loss: 0.7049, Val Accuracy: 75.27%\n",
      "Epoch [26/100], Train Loss: 0.6909, Train Accuracy: 76.34%, Val Loss: 0.6986, Val Accuracy: 75.51%\n",
      "Epoch [27/100], Train Loss: 0.6839, Train Accuracy: 76.63%, Val Loss: 0.6922, Val Accuracy: 76.13%\n",
      "Epoch [28/100], Train Loss: 0.6774, Train Accuracy: 76.96%, Val Loss: 0.6841, Val Accuracy: 76.31%\n",
      "Epoch [29/100], Train Loss: 0.6709, Train Accuracy: 77.27%, Val Loss: 0.6784, Val Accuracy: 76.67%\n",
      "Epoch [30/100], Train Loss: 0.6651, Train Accuracy: 77.49%, Val Loss: 0.6730, Val Accuracy: 76.69%\n",
      "Epoch [31/100], Train Loss: 0.6595, Train Accuracy: 77.62%, Val Loss: 0.6674, Val Accuracy: 76.89%\n",
      "Epoch [32/100], Train Loss: 0.6541, Train Accuracy: 77.94%, Val Loss: 0.6622, Val Accuracy: 77.12%\n",
      "Epoch [33/100], Train Loss: 0.6490, Train Accuracy: 78.20%, Val Loss: 0.6585, Val Accuracy: 77.50%\n",
      "Epoch [34/100], Train Loss: 0.6443, Train Accuracy: 78.30%, Val Loss: 0.6529, Val Accuracy: 77.69%\n",
      "Epoch [35/100], Train Loss: 0.6396, Train Accuracy: 78.49%, Val Loss: 0.6495, Val Accuracy: 77.69%\n",
      "Epoch [36/100], Train Loss: 0.6350, Train Accuracy: 78.55%, Val Loss: 0.6448, Val Accuracy: 77.79%\n",
      "Epoch [37/100], Train Loss: 0.6307, Train Accuracy: 78.82%, Val Loss: 0.6414, Val Accuracy: 77.95%\n",
      "Epoch [38/100], Train Loss: 0.6266, Train Accuracy: 78.94%, Val Loss: 0.6367, Val Accuracy: 78.38%\n",
      "Epoch [39/100], Train Loss: 0.6228, Train Accuracy: 79.06%, Val Loss: 0.6329, Val Accuracy: 78.47%\n",
      "Epoch [40/100], Train Loss: 0.6190, Train Accuracy: 79.13%, Val Loss: 0.6299, Val Accuracy: 78.52%\n",
      "Epoch [41/100], Train Loss: 0.6153, Train Accuracy: 79.30%, Val Loss: 0.6260, Val Accuracy: 78.57%\n",
      "Epoch [42/100], Train Loss: 0.6119, Train Accuracy: 79.31%, Val Loss: 0.6225, Val Accuracy: 78.67%\n",
      "Epoch [43/100], Train Loss: 0.6085, Train Accuracy: 79.45%, Val Loss: 0.6201, Val Accuracy: 78.75%\n",
      "Epoch [44/100], Train Loss: 0.6052, Train Accuracy: 79.54%, Val Loss: 0.6173, Val Accuracy: 78.83%\n",
      "Epoch [45/100], Train Loss: 0.6022, Train Accuracy: 79.63%, Val Loss: 0.6134, Val Accuracy: 79.03%\n",
      "Epoch [46/100], Train Loss: 0.5990, Train Accuracy: 79.77%, Val Loss: 0.6117, Val Accuracy: 79.03%\n",
      "Epoch [47/100], Train Loss: 0.5962, Train Accuracy: 79.83%, Val Loss: 0.6082, Val Accuracy: 79.17%\n",
      "Epoch [48/100], Train Loss: 0.5935, Train Accuracy: 79.91%, Val Loss: 0.6052, Val Accuracy: 79.30%\n",
      "Epoch [49/100], Train Loss: 0.5907, Train Accuracy: 79.96%, Val Loss: 0.6031, Val Accuracy: 79.25%\n",
      "Epoch [50/100], Train Loss: 0.5881, Train Accuracy: 80.09%, Val Loss: 0.6017, Val Accuracy: 79.51%\n",
      "Epoch [51/100], Train Loss: 0.5855, Train Accuracy: 80.21%, Val Loss: 0.5988, Val Accuracy: 79.50%\n",
      "Epoch [52/100], Train Loss: 0.5831, Train Accuracy: 80.23%, Val Loss: 0.5971, Val Accuracy: 79.59%\n",
      "Epoch [53/100], Train Loss: 0.5805, Train Accuracy: 80.40%, Val Loss: 0.5937, Val Accuracy: 79.85%\n",
      "Epoch [54/100], Train Loss: 0.5784, Train Accuracy: 80.38%, Val Loss: 0.5914, Val Accuracy: 79.88%\n",
      "Epoch [55/100], Train Loss: 0.5762, Train Accuracy: 80.45%, Val Loss: 0.5909, Val Accuracy: 79.90%\n",
      "Epoch [56/100], Train Loss: 0.5736, Train Accuracy: 80.56%, Val Loss: 0.5892, Val Accuracy: 79.97%\n",
      "Epoch [57/100], Train Loss: 0.5716, Train Accuracy: 80.56%, Val Loss: 0.5868, Val Accuracy: 79.90%\n",
      "Epoch [58/100], Train Loss: 0.5697, Train Accuracy: 80.67%, Val Loss: 0.5842, Val Accuracy: 80.02%\n",
      "Epoch [59/100], Train Loss: 0.5677, Train Accuracy: 80.73%, Val Loss: 0.5825, Val Accuracy: 80.12%\n",
      "Epoch [60/100], Train Loss: 0.5657, Train Accuracy: 80.78%, Val Loss: 0.5807, Val Accuracy: 80.20%\n",
      "Epoch [61/100], Train Loss: 0.5638, Train Accuracy: 80.87%, Val Loss: 0.5802, Val Accuracy: 80.28%\n",
      "Epoch [62/100], Train Loss: 0.5619, Train Accuracy: 80.85%, Val Loss: 0.5783, Val Accuracy: 80.14%\n",
      "Epoch [63/100], Train Loss: 0.5602, Train Accuracy: 80.94%, Val Loss: 0.5749, Val Accuracy: 80.35%\n",
      "Epoch [64/100], Train Loss: 0.5582, Train Accuracy: 81.05%, Val Loss: 0.5751, Val Accuracy: 80.27%\n",
      "Epoch [65/100], Train Loss: 0.5567, Train Accuracy: 81.06%, Val Loss: 0.5730, Val Accuracy: 80.34%\n",
      "Epoch [66/100], Train Loss: 0.5548, Train Accuracy: 81.12%, Val Loss: 0.5730, Val Accuracy: 80.35%\n",
      "Epoch [67/100], Train Loss: 0.5534, Train Accuracy: 81.14%, Val Loss: 0.5696, Val Accuracy: 80.52%\n",
      "Epoch [68/100], Train Loss: 0.5517, Train Accuracy: 81.24%, Val Loss: 0.5682, Val Accuracy: 80.71%\n",
      "Epoch [69/100], Train Loss: 0.5503, Train Accuracy: 81.29%, Val Loss: 0.5669, Val Accuracy: 80.60%\n",
      "Epoch [70/100], Train Loss: 0.5486, Train Accuracy: 81.36%, Val Loss: 0.5664, Val Accuracy: 80.58%\n",
      "Epoch [71/100], Train Loss: 0.5472, Train Accuracy: 81.37%, Val Loss: 0.5640, Val Accuracy: 80.68%\n",
      "Epoch [72/100], Train Loss: 0.5459, Train Accuracy: 81.40%, Val Loss: 0.5632, Val Accuracy: 80.80%\n",
      "Epoch [73/100], Train Loss: 0.5446, Train Accuracy: 81.46%, Val Loss: 0.5618, Val Accuracy: 80.89%\n",
      "Epoch [74/100], Train Loss: 0.5433, Train Accuracy: 81.47%, Val Loss: 0.5606, Val Accuracy: 80.86%\n",
      "Epoch [75/100], Train Loss: 0.5417, Train Accuracy: 81.61%, Val Loss: 0.5594, Val Accuracy: 80.83%\n",
      "Epoch [76/100], Train Loss: 0.5404, Train Accuracy: 81.61%, Val Loss: 0.5593, Val Accuracy: 80.78%\n",
      "Epoch [77/100], Train Loss: 0.5393, Train Accuracy: 81.62%, Val Loss: 0.5562, Val Accuracy: 81.07%\n",
      "Epoch [78/100], Train Loss: 0.5381, Train Accuracy: 81.65%, Val Loss: 0.5553, Val Accuracy: 81.08%\n",
      "Epoch [79/100], Train Loss: 0.5367, Train Accuracy: 81.64%, Val Loss: 0.5547, Val Accuracy: 81.13%\n",
      "Epoch [80/100], Train Loss: 0.5355, Train Accuracy: 81.81%, Val Loss: 0.5540, Val Accuracy: 81.03%\n",
      "Epoch [81/100], Train Loss: 0.5344, Train Accuracy: 81.81%, Val Loss: 0.5522, Val Accuracy: 81.20%\n",
      "Epoch [82/100], Train Loss: 0.5334, Train Accuracy: 81.77%, Val Loss: 0.5532, Val Accuracy: 81.08%\n",
      "Epoch [83/100], Train Loss: 0.5323, Train Accuracy: 81.85%, Val Loss: 0.5513, Val Accuracy: 81.22%\n",
      "Epoch [84/100], Train Loss: 0.5311, Train Accuracy: 81.90%, Val Loss: 0.5497, Val Accuracy: 81.33%\n",
      "Epoch [85/100], Train Loss: 0.5300, Train Accuracy: 81.97%, Val Loss: 0.5493, Val Accuracy: 81.30%\n",
      "Epoch [86/100], Train Loss: 0.5289, Train Accuracy: 81.90%, Val Loss: 0.5477, Val Accuracy: 81.33%\n",
      "Epoch [87/100], Train Loss: 0.5280, Train Accuracy: 81.96%, Val Loss: 0.5479, Val Accuracy: 81.47%\n",
      "Epoch [88/100], Train Loss: 0.5270, Train Accuracy: 82.01%, Val Loss: 0.5458, Val Accuracy: 81.32%\n",
      "Epoch [89/100], Train Loss: 0.5259, Train Accuracy: 82.06%, Val Loss: 0.5468, Val Accuracy: 81.38%\n",
      "Epoch [90/100], Train Loss: 0.5251, Train Accuracy: 82.10%, Val Loss: 0.5447, Val Accuracy: 81.53%\n",
      "Epoch [91/100], Train Loss: 0.5241, Train Accuracy: 82.21%, Val Loss: 0.5437, Val Accuracy: 81.59%\n",
      "Epoch [92/100], Train Loss: 0.5231, Train Accuracy: 82.20%, Val Loss: 0.5427, Val Accuracy: 81.52%\n",
      "Epoch [93/100], Train Loss: 0.5223, Train Accuracy: 82.24%, Val Loss: 0.5417, Val Accuracy: 81.71%\n",
      "Epoch [94/100], Train Loss: 0.5215, Train Accuracy: 82.15%, Val Loss: 0.5407, Val Accuracy: 81.64%\n",
      "Epoch [95/100], Train Loss: 0.5205, Train Accuracy: 82.26%, Val Loss: 0.5417, Val Accuracy: 81.55%\n",
      "Epoch [96/100], Train Loss: 0.5196, Train Accuracy: 82.22%, Val Loss: 0.5390, Val Accuracy: 81.85%\n",
      "Epoch [97/100], Train Loss: 0.5188, Train Accuracy: 82.33%, Val Loss: 0.5389, Val Accuracy: 81.78%\n",
      "Epoch [98/100], Train Loss: 0.5180, Train Accuracy: 82.30%, Val Loss: 0.5375, Val Accuracy: 81.86%\n",
      "Epoch [99/100], Train Loss: 0.5170, Train Accuracy: 82.26%, Val Loss: 0.5366, Val Accuracy: 81.75%\n",
      "Epoch [100/100], Train Loss: 0.5162, Train Accuracy: 82.43%, Val Loss: 0.5371, Val Accuracy: 81.86%\n",
      "Model Accutacy:81.18\n",
      "8 100 small_random relu sgd None 0.0 start\n",
      "Epoch [1/100], Train Loss: 2.2446, Train Accuracy: 12.07%, Val Loss: 2.1368, Val Accuracy: 9.09%\n",
      "Epoch [2/100], Train Loss: 1.9918, Train Accuracy: 16.87%, Val Loss: 1.8213, Val Accuracy: 32.57%\n",
      "Epoch [3/100], Train Loss: 1.6050, Train Accuracy: 46.99%, Val Loss: 1.4141, Val Accuracy: 56.26%\n",
      "Epoch [4/100], Train Loss: 1.2611, Train Accuracy: 60.45%, Val Loss: 1.1566, Val Accuracy: 62.49%\n",
      "Epoch [5/100], Train Loss: 1.0665, Train Accuracy: 65.16%, Val Loss: 1.0127, Val Accuracy: 65.88%\n",
      "Epoch [6/100], Train Loss: 0.9496, Train Accuracy: 67.58%, Val Loss: 0.9189, Val Accuracy: 67.98%\n",
      "Epoch [7/100], Train Loss: 0.8719, Train Accuracy: 69.47%, Val Loss: 0.8563, Val Accuracy: 69.25%\n",
      "Epoch [8/100], Train Loss: 0.8184, Train Accuracy: 71.21%, Val Loss: 0.8116, Val Accuracy: 70.97%\n",
      "Epoch [9/100], Train Loss: 0.7782, Train Accuracy: 72.71%, Val Loss: 0.7760, Val Accuracy: 72.37%\n",
      "Epoch [10/100], Train Loss: 0.7458, Train Accuracy: 73.99%, Val Loss: 0.7479, Val Accuracy: 73.58%\n",
      "Epoch [11/100], Train Loss: 0.7186, Train Accuracy: 75.30%, Val Loss: 0.7224, Val Accuracy: 74.83%\n",
      "Epoch [12/100], Train Loss: 0.6954, Train Accuracy: 76.15%, Val Loss: 0.7007, Val Accuracy: 75.49%\n",
      "Epoch [13/100], Train Loss: 0.6752, Train Accuracy: 76.89%, Val Loss: 0.6817, Val Accuracy: 76.31%\n",
      "Epoch [14/100], Train Loss: 0.6577, Train Accuracy: 77.67%, Val Loss: 0.6650, Val Accuracy: 76.78%\n",
      "Epoch [15/100], Train Loss: 0.6422, Train Accuracy: 78.23%, Val Loss: 0.6507, Val Accuracy: 77.27%\n",
      "Epoch [16/100], Train Loss: 0.6283, Train Accuracy: 78.75%, Val Loss: 0.6374, Val Accuracy: 77.72%\n",
      "Epoch [17/100], Train Loss: 0.6161, Train Accuracy: 79.01%, Val Loss: 0.6256, Val Accuracy: 78.35%\n",
      "Epoch [18/100], Train Loss: 0.6053, Train Accuracy: 79.53%, Val Loss: 0.6157, Val Accuracy: 78.82%\n",
      "Epoch [19/100], Train Loss: 0.5955, Train Accuracy: 79.76%, Val Loss: 0.6078, Val Accuracy: 79.07%\n",
      "Epoch [20/100], Train Loss: 0.5868, Train Accuracy: 80.05%, Val Loss: 0.5983, Val Accuracy: 79.30%\n",
      "Epoch [21/100], Train Loss: 0.5788, Train Accuracy: 80.31%, Val Loss: 0.5909, Val Accuracy: 79.52%\n",
      "Epoch [22/100], Train Loss: 0.5715, Train Accuracy: 80.57%, Val Loss: 0.5837, Val Accuracy: 79.88%\n",
      "Epoch [23/100], Train Loss: 0.5653, Train Accuracy: 80.71%, Val Loss: 0.5778, Val Accuracy: 80.28%\n",
      "Epoch [24/100], Train Loss: 0.5592, Train Accuracy: 80.92%, Val Loss: 0.5719, Val Accuracy: 80.28%\n",
      "Epoch [25/100], Train Loss: 0.5534, Train Accuracy: 81.10%, Val Loss: 0.5668, Val Accuracy: 80.57%\n",
      "Epoch [26/100], Train Loss: 0.5483, Train Accuracy: 81.27%, Val Loss: 0.5622, Val Accuracy: 80.74%\n",
      "Epoch [27/100], Train Loss: 0.5436, Train Accuracy: 81.44%, Val Loss: 0.5581, Val Accuracy: 80.71%\n",
      "Epoch [28/100], Train Loss: 0.5388, Train Accuracy: 81.52%, Val Loss: 0.5544, Val Accuracy: 81.01%\n",
      "Epoch [29/100], Train Loss: 0.5348, Train Accuracy: 81.69%, Val Loss: 0.5500, Val Accuracy: 80.98%\n",
      "Epoch [30/100], Train Loss: 0.5309, Train Accuracy: 81.83%, Val Loss: 0.5454, Val Accuracy: 81.27%\n",
      "Epoch [31/100], Train Loss: 0.5271, Train Accuracy: 81.95%, Val Loss: 0.5424, Val Accuracy: 81.32%\n",
      "Epoch [32/100], Train Loss: 0.5235, Train Accuracy: 81.98%, Val Loss: 0.5402, Val Accuracy: 81.20%\n",
      "Epoch [33/100], Train Loss: 0.5203, Train Accuracy: 82.13%, Val Loss: 0.5355, Val Accuracy: 81.47%\n",
      "Epoch [34/100], Train Loss: 0.5171, Train Accuracy: 82.20%, Val Loss: 0.5335, Val Accuracy: 81.58%\n",
      "Epoch [35/100], Train Loss: 0.5140, Train Accuracy: 82.27%, Val Loss: 0.5293, Val Accuracy: 81.72%\n",
      "Epoch [36/100], Train Loss: 0.5110, Train Accuracy: 82.35%, Val Loss: 0.5286, Val Accuracy: 81.83%\n",
      "Epoch [37/100], Train Loss: 0.5084, Train Accuracy: 82.48%, Val Loss: 0.5267, Val Accuracy: 81.86%\n",
      "Epoch [38/100], Train Loss: 0.5059, Train Accuracy: 82.55%, Val Loss: 0.5223, Val Accuracy: 81.88%\n",
      "Epoch [39/100], Train Loss: 0.5033, Train Accuracy: 82.59%, Val Loss: 0.5219, Val Accuracy: 82.06%\n",
      "Epoch [40/100], Train Loss: 0.5010, Train Accuracy: 82.70%, Val Loss: 0.5180, Val Accuracy: 82.03%\n",
      "Epoch [41/100], Train Loss: 0.4986, Train Accuracy: 82.71%, Val Loss: 0.5179, Val Accuracy: 82.24%\n",
      "Epoch [42/100], Train Loss: 0.4965, Train Accuracy: 82.74%, Val Loss: 0.5148, Val Accuracy: 82.17%\n",
      "Epoch [43/100], Train Loss: 0.4945, Train Accuracy: 82.83%, Val Loss: 0.5121, Val Accuracy: 82.16%\n",
      "Epoch [44/100], Train Loss: 0.4925, Train Accuracy: 82.90%, Val Loss: 0.5096, Val Accuracy: 82.40%\n",
      "Epoch [45/100], Train Loss: 0.4906, Train Accuracy: 83.00%, Val Loss: 0.5089, Val Accuracy: 82.25%\n",
      "Epoch [46/100], Train Loss: 0.4884, Train Accuracy: 83.03%, Val Loss: 0.5079, Val Accuracy: 82.31%\n",
      "Epoch [47/100], Train Loss: 0.4869, Train Accuracy: 83.12%, Val Loss: 0.5056, Val Accuracy: 82.53%\n",
      "Epoch [48/100], Train Loss: 0.4851, Train Accuracy: 83.17%, Val Loss: 0.5026, Val Accuracy: 82.57%\n",
      "Epoch [49/100], Train Loss: 0.4835, Train Accuracy: 83.22%, Val Loss: 0.5019, Val Accuracy: 82.50%\n",
      "Epoch [50/100], Train Loss: 0.4818, Train Accuracy: 83.24%, Val Loss: 0.4994, Val Accuracy: 82.70%\n",
      "Epoch [51/100], Train Loss: 0.4801, Train Accuracy: 83.43%, Val Loss: 0.4980, Val Accuracy: 82.79%\n",
      "Epoch [52/100], Train Loss: 0.4788, Train Accuracy: 83.39%, Val Loss: 0.4975, Val Accuracy: 82.55%\n",
      "Epoch [53/100], Train Loss: 0.4774, Train Accuracy: 83.53%, Val Loss: 0.4959, Val Accuracy: 82.64%\n",
      "Epoch [54/100], Train Loss: 0.4758, Train Accuracy: 83.53%, Val Loss: 0.4950, Val Accuracy: 82.92%\n",
      "Epoch [55/100], Train Loss: 0.4747, Train Accuracy: 83.60%, Val Loss: 0.4938, Val Accuracy: 82.81%\n",
      "Epoch [56/100], Train Loss: 0.4733, Train Accuracy: 83.59%, Val Loss: 0.4929, Val Accuracy: 82.70%\n",
      "Epoch [57/100], Train Loss: 0.4719, Train Accuracy: 83.67%, Val Loss: 0.4932, Val Accuracy: 83.03%\n",
      "Epoch [58/100], Train Loss: 0.4707, Train Accuracy: 83.74%, Val Loss: 0.4904, Val Accuracy: 82.78%\n",
      "Epoch [59/100], Train Loss: 0.4695, Train Accuracy: 83.75%, Val Loss: 0.4888, Val Accuracy: 82.88%\n",
      "Epoch [60/100], Train Loss: 0.4685, Train Accuracy: 83.79%, Val Loss: 0.4888, Val Accuracy: 82.87%\n",
      "Epoch [61/100], Train Loss: 0.4671, Train Accuracy: 83.83%, Val Loss: 0.4867, Val Accuracy: 82.99%\n",
      "Epoch [62/100], Train Loss: 0.4661, Train Accuracy: 83.89%, Val Loss: 0.4851, Val Accuracy: 83.09%\n",
      "Epoch [63/100], Train Loss: 0.4647, Train Accuracy: 83.91%, Val Loss: 0.4853, Val Accuracy: 83.23%\n",
      "Epoch [64/100], Train Loss: 0.4640, Train Accuracy: 83.95%, Val Loss: 0.4840, Val Accuracy: 83.03%\n",
      "Epoch [65/100], Train Loss: 0.4627, Train Accuracy: 83.99%, Val Loss: 0.4825, Val Accuracy: 83.22%\n",
      "Epoch [66/100], Train Loss: 0.4618, Train Accuracy: 84.08%, Val Loss: 0.4812, Val Accuracy: 83.23%\n",
      "Epoch [67/100], Train Loss: 0.4608, Train Accuracy: 84.09%, Val Loss: 0.4803, Val Accuracy: 83.29%\n",
      "Epoch [68/100], Train Loss: 0.4598, Train Accuracy: 84.09%, Val Loss: 0.4791, Val Accuracy: 83.41%\n",
      "Epoch [69/100], Train Loss: 0.4588, Train Accuracy: 84.12%, Val Loss: 0.4786, Val Accuracy: 83.40%\n",
      "Epoch [70/100], Train Loss: 0.4579, Train Accuracy: 84.14%, Val Loss: 0.4776, Val Accuracy: 83.41%\n",
      "Epoch [71/100], Train Loss: 0.4569, Train Accuracy: 84.20%, Val Loss: 0.4773, Val Accuracy: 83.47%\n",
      "Epoch [72/100], Train Loss: 0.4560, Train Accuracy: 84.24%, Val Loss: 0.4779, Val Accuracy: 83.46%\n",
      "Epoch [73/100], Train Loss: 0.4553, Train Accuracy: 84.25%, Val Loss: 0.4751, Val Accuracy: 83.46%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _training_schedule()\n",
      "Cell \u001b[1;32mIn[23], line 44\u001b[0m, in \u001b[0;36m_training_schedule\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mprint\u001b[39m(hidden_size, epochs, init, active, optimize, schedule, weight_decay, \u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m write_spec_to_file()\n\u001b[1;32m---> 44\u001b[0m train(model\u001b[39m=\u001b[39;49mmodel, opt\u001b[39m=\u001b[39;49moptm, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49mepochs, learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     45\u001b[0m       trainloader\u001b[39m=\u001b[39;49mtrainloader, valloader\u001b[39m=\u001b[39;49mvalloader, criterion\u001b[39m=\u001b[39;49mcriterion, sched\u001b[39m=\u001b[39;49mschd, weight_decay\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[0;32m     46\u001b[0m result \u001b[39m=\u001b[39m test(model\u001b[39m=\u001b[39mmodel, device\u001b[39m=\u001b[39mdevice, testloader\u001b[39m=\u001b[39mtestloader)\n\u001b[0;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(FILE_PATH,\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, opt, device, epochs, learning_rate, trainloader, valloader, criterion, sched, weight_decay)\u001b[0m\n\u001b[0;32m     31\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     32\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m valloader:\n\u001b[0;32m     34\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39minput_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32me:\\Users\\Rex\\miniconda3\\envs\\py310_pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_training_schedule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
