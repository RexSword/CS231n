{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Requirement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Implement the code for the 2-layer neural networks in CS231n \n",
    "2021 version with PyTorch (or TensorFlow). \n",
    "\n",
    "• Once you have the code (regardless of which framework you \n",
    "choose above), you will apply your own data.  The training and test \n",
    "dataset is 80%:20%.\n",
    "\n",
    "• You need to run the code with the following hyperparameter \n",
    "settings:\n",
    "\n",
    "✓ Activation function: tanh, ReLU\n",
    "\n",
    "✓ Data preprocessing\n",
    "\n",
    "✓ Initial weights: small random number, Xavier or Kaiming/MSRA \n",
    "Initialization\n",
    "\n",
    "✓ Loss function: without or with the regularization term \n",
    "(L2), λ = \n",
    "0.001 or 0.0001\n",
    "$$ E(w) = \\frac{1}{N}\\sum^{N}_{c=1}[𝑓(X^c, w) −y^c]^2 \n",
    " + \\lambda[\\sum^{p}_{i=0}(w^{o}_{i})^2\n",
    " + \\sum_{i=1}^{p}\\sum_{j=0}^{m}(w_{ij}^H)^2]\n",
    "$$\n",
    "✓ Optimizer: gradient descent, Momentum, Adam\n",
    "\n",
    "✓ Learning epochs: 100, 200, 300\n",
    "\n",
    "✓ Amount of hidden nodes: 5, 8, 11\n",
    "\n",
    "✓ Learning rate decay schedule: none and cosine\n",
    "\n",
    "✓ Ensembles: top 3 models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, Generator\n",
    "from torch.utils.data import DataLoader, Dataset, sampler, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actives = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"tanh\": nn.Tanh\n",
    "}\n",
    "inits = {\n",
    "    \"small_random\": lambda x: nn.init.normal_(tensor=x, mean=0, std=0.01),\n",
    "    \"xavier\": lambda x: nn.init.xavier_uniform_(tensor=x) if len(x.shape) > 1 else None,\n",
    "    \"kaiming\": lambda x: nn.init.kaiming_uniform_(tensor=x, nonlinearity='relu') if len(x.shape) > 1 else None\n",
    "}\n",
    "optims = {\n",
    "    \"sgd\": optim.SGD,\n",
    "    \"momentum\": lambda param, lr: optim.SGD(params=param, lr=lr, momentum=0.9),\n",
    "    \"adam\": optim.Adam\n",
    "}\n",
    "schedulers={\n",
    "    \"cos\":lambda opt:torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=opt, T_max=200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "class TwoLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, init_method:Callable, active_func:nn.modules.module.Module) -> None:\n",
    "        super(TwoLayerNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        ## first layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        ## activation\n",
    "        self.active_func = active_func()\n",
    "        ## initialize\n",
    "        for param in self.parameters():\n",
    "            init_method(param)\n",
    "        ## second layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.active_func(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: TwoLayerNetwork, opt: nn.Module, device: str, epochs: int, learning_rate: float, trainloader: DataLoader, valloader: DataLoader, criterion: nn.modules.loss._Loss, sched: optim.lr_scheduler._LRScheduler):\n",
    "    model.to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "    scheduler = sched(optimizer) if sched else None\n",
    "    if epochs < 1:\n",
    "        raise ValueError(\"Invalid epoch!!\")\n",
    "    else:\n",
    "        epochs = int(epochs)\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == y).sum().item()\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "        train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in valloader:\n",
    "                X = X.view(-1, model.input_size).to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == y).sum().item()\n",
    "            val_loss /= len(valloader.dataset)\n",
    "            val_accuracy = 100. * val_correct / len(valloader.dataset)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Print epoch statistics\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'\n",
    "              .format(epoch+1, epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.5380374862183\n"
     ]
    }
   ],
   "source": [
    "def test(model:nn.Module, device:str, testloader:DataLoader):\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in testloader:\n",
    "            X = X.view(-1, model.input_size).to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "        val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
    "        print(val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class HotelReservationDataset(Dataset):\n",
    "    \"\"\"Hotel Reservation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        reservations = pd.read_csv(csv_path)\n",
    "        self.labels_of_columns = dict()\n",
    "        for col in map(lambda x: x[0], filter(lambda x:x[1]==\"O\", reservations.dtypes.items())):\n",
    "            d = dict((j, i) for i, j in enumerate(reservations[col].value_counts().index))\n",
    "            self.labels_of_columns[col] = d.keys()\n",
    "            reservations[col]=reservations[col].apply(d.__getitem__)\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.features = torch.from_numpy(reservations.iloc[:, 1:-1].to_numpy(dtype=np.float32))\n",
    "        self.labels = torch.reshape(torch.tensor(reservations.iloc[:, -1:].to_numpy()), shape=(len(self.features),))\n",
    "        print(self.labels.size())\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36275])\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "dataset = HotelReservationDataset(\n",
    "    csv_path=r\"D:\\dataset\\archive\\Hotel Reservations.csv\", root_dir=\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "\n",
    "# train test split\n",
    "train_count = int(0.7 * len(dataset))\n",
    "valid_count = int(0.2 * len(dataset))\n",
    "test_count = len(dataset) - train_count - valid_count\n",
    "print(train_count, valid_count, test_count)\n",
    "trainset, valset, testset = random_split(\n",
    "    dataset, (train_count, valid_count, test_count), Generator().manual_seed(42))\n",
    "# set loaders\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 100 small_random relu sgd cos\n",
      "5 100 small_random relu momentum cos\n",
      "5 100 small_random relu adam cos\n",
      "5 100 small_random tanh sgd cos\n",
      "5 100 small_random tanh momentum cos\n",
      "5 100 small_random tanh adam cos\n",
      "5 100 xavier relu sgd cos\n",
      "5 100 xavier relu momentum cos\n",
      "5 100 xavier relu adam cos\n",
      "5 100 xavier tanh sgd cos\n",
      "5 100 xavier tanh momentum cos\n",
      "5 100 xavier tanh adam cos\n",
      "5 100 kaiming relu sgd cos\n",
      "5 100 kaiming relu momentum cos\n",
      "5 100 kaiming relu adam cos\n",
      "5 100 kaiming tanh sgd cos\n",
      "5 100 kaiming tanh momentum cos\n",
      "5 100 kaiming tanh adam cos\n",
      "5 200 small_random relu sgd cos\n",
      "5 200 small_random relu momentum cos\n",
      "5 200 small_random relu adam cos\n",
      "5 200 small_random tanh sgd cos\n",
      "5 200 small_random tanh momentum cos\n",
      "5 200 small_random tanh adam cos\n",
      "5 200 xavier relu sgd cos\n",
      "5 200 xavier relu momentum cos\n",
      "5 200 xavier relu adam cos\n",
      "5 200 xavier tanh sgd cos\n",
      "5 200 xavier tanh momentum cos\n",
      "5 200 xavier tanh adam cos\n",
      "5 200 kaiming relu sgd cos\n",
      "5 200 kaiming relu momentum cos\n",
      "5 200 kaiming relu adam cos\n",
      "5 200 kaiming tanh sgd cos\n",
      "5 200 kaiming tanh momentum cos\n",
      "5 200 kaiming tanh adam cos\n",
      "5 300 small_random relu sgd cos\n",
      "5 300 small_random relu momentum cos\n",
      "5 300 small_random relu adam cos\n",
      "5 300 small_random tanh sgd cos\n",
      "5 300 small_random tanh momentum cos\n",
      "5 300 small_random tanh adam cos\n",
      "5 300 xavier relu sgd cos\n",
      "5 300 xavier relu momentum cos\n",
      "5 300 xavier relu adam cos\n",
      "5 300 xavier tanh sgd cos\n",
      "5 300 xavier tanh momentum cos\n",
      "5 300 xavier tanh adam cos\n",
      "5 300 kaiming relu sgd cos\n",
      "5 300 kaiming relu momentum cos\n",
      "5 300 kaiming relu adam cos\n",
      "5 300 kaiming tanh sgd cos\n",
      "5 300 kaiming tanh momentum cos\n",
      "5 300 kaiming tanh adam cos\n",
      "8 100 small_random relu sgd cos\n",
      "8 100 small_random relu momentum cos\n",
      "8 100 small_random relu adam cos\n",
      "8 100 small_random tanh sgd cos\n",
      "8 100 small_random tanh momentum cos\n",
      "8 100 small_random tanh adam cos\n",
      "8 100 xavier relu sgd cos\n",
      "8 100 xavier relu momentum cos\n",
      "8 100 xavier relu adam cos\n",
      "8 100 xavier tanh sgd cos\n",
      "8 100 xavier tanh momentum cos\n",
      "8 100 xavier tanh adam cos\n",
      "8 100 kaiming relu sgd cos\n",
      "8 100 kaiming relu momentum cos\n",
      "8 100 kaiming relu adam cos\n",
      "8 100 kaiming tanh sgd cos\n",
      "8 100 kaiming tanh momentum cos\n",
      "8 100 kaiming tanh adam cos\n",
      "8 200 small_random relu sgd cos\n",
      "8 200 small_random relu momentum cos\n",
      "8 200 small_random relu adam cos\n",
      "8 200 small_random tanh sgd cos\n",
      "8 200 small_random tanh momentum cos\n",
      "8 200 small_random tanh adam cos\n",
      "8 200 xavier relu sgd cos\n",
      "8 200 xavier relu momentum cos\n",
      "8 200 xavier relu adam cos\n",
      "8 200 xavier tanh sgd cos\n",
      "8 200 xavier tanh momentum cos\n",
      "8 200 xavier tanh adam cos\n",
      "8 200 kaiming relu sgd cos\n",
      "8 200 kaiming relu momentum cos\n",
      "8 200 kaiming relu adam cos\n",
      "8 200 kaiming tanh sgd cos\n",
      "8 200 kaiming tanh momentum cos\n",
      "8 200 kaiming tanh adam cos\n",
      "8 300 small_random relu sgd cos\n",
      "8 300 small_random relu momentum cos\n",
      "8 300 small_random relu adam cos\n",
      "8 300 small_random tanh sgd cos\n",
      "8 300 small_random tanh momentum cos\n",
      "8 300 small_random tanh adam cos\n",
      "8 300 xavier relu sgd cos\n",
      "8 300 xavier relu momentum cos\n",
      "8 300 xavier relu adam cos\n",
      "8 300 xavier tanh sgd cos\n",
      "8 300 xavier tanh momentum cos\n",
      "8 300 xavier tanh adam cos\n",
      "8 300 kaiming relu sgd cos\n",
      "8 300 kaiming relu momentum cos\n",
      "8 300 kaiming relu adam cos\n",
      "8 300 kaiming tanh sgd cos\n",
      "8 300 kaiming tanh momentum cos\n",
      "8 300 kaiming tanh adam cos\n",
      "11 100 small_random relu sgd cos\n",
      "11 100 small_random relu momentum cos\n",
      "11 100 small_random relu adam cos\n",
      "11 100 small_random tanh sgd cos\n",
      "11 100 small_random tanh momentum cos\n",
      "11 100 small_random tanh adam cos\n",
      "11 100 xavier relu sgd cos\n",
      "11 100 xavier relu momentum cos\n",
      "11 100 xavier relu adam cos\n",
      "11 100 xavier tanh sgd cos\n",
      "11 100 xavier tanh momentum cos\n",
      "11 100 xavier tanh adam cos\n",
      "11 100 kaiming relu sgd cos\n",
      "11 100 kaiming relu momentum cos\n",
      "11 100 kaiming relu adam cos\n",
      "11 100 kaiming tanh sgd cos\n",
      "11 100 kaiming tanh momentum cos\n",
      "11 100 kaiming tanh adam cos\n",
      "11 200 small_random relu sgd cos\n",
      "11 200 small_random relu momentum cos\n",
      "11 200 small_random relu adam cos\n",
      "11 200 small_random tanh sgd cos\n",
      "11 200 small_random tanh momentum cos\n",
      "11 200 small_random tanh adam cos\n",
      "11 200 xavier relu sgd cos\n",
      "11 200 xavier relu momentum cos\n",
      "11 200 xavier relu adam cos\n",
      "11 200 xavier tanh sgd cos\n",
      "11 200 xavier tanh momentum cos\n",
      "11 200 xavier tanh adam cos\n",
      "11 200 kaiming relu sgd cos\n",
      "11 200 kaiming relu momentum cos\n",
      "11 200 kaiming relu adam cos\n",
      "11 200 kaiming tanh sgd cos\n",
      "11 200 kaiming tanh momentum cos\n",
      "11 200 kaiming tanh adam cos\n",
      "11 300 small_random relu sgd cos\n",
      "11 300 small_random relu momentum cos\n",
      "11 300 small_random relu adam cos\n",
      "11 300 small_random tanh sgd cos\n",
      "11 300 small_random tanh momentum cos\n",
      "11 300 small_random tanh adam cos\n",
      "11 300 xavier relu sgd cos\n",
      "11 300 xavier relu momentum cos\n",
      "11 300 xavier relu adam cos\n",
      "11 300 xavier tanh sgd cos\n",
      "11 300 xavier tanh momentum cos\n",
      "11 300 xavier tanh adam cos\n",
      "11 300 kaiming relu sgd cos\n",
      "11 300 kaiming relu momentum cos\n",
      "11 300 kaiming relu adam cos\n",
      "11 300 kaiming tanh sgd cos\n",
      "11 300 kaiming tanh momentum cos\n",
      "11 300 kaiming tanh adam cos\n"
     ]
    }
   ],
   "source": [
    "def training_schedule():\n",
    "    # processor\n",
    "    device = \"cuda\" if torch.cuda.is_available(\n",
    "    ) else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # ✓ Loss function: without or with L2, λ = 0.001 or 0.0001\n",
    "\n",
    "    # hyper parameters\n",
    "    input_size = len(dataset[0][0])\n",
    "    output_size = 2\n",
    "    learning_rate = 0.1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # ✓ Amount of hidden nodes: 5, 8, 11\n",
    "    for hidden_size in (5, 8, 11):\n",
    "        # ✓ Learning epochs: 100, 200, 300\n",
    "        for epochs in (100, 200, 300):\n",
    "            # Create model, optimizer, scheduler\n",
    "            for (init, method) in inits.items():\n",
    "                for (active, func) in actives.items():\n",
    "                    # ✓ Activation function: tanh, ReLU\n",
    "                    # ✓ Initial weights: small random number, Xavier or Kaiming/MSRA Initialization\n",
    "                    model = TwoLayerNetwork(input_size, hidden_size, output_size,\n",
    "                                            init_method=method, active_func=func).to(device)\n",
    "                    # ✓ Optimizer: gradient descent, Momentum, Adam\n",
    "                    for (optimi, zer) in optims.items():\n",
    "                        # ✓ Learning rate decay schedule: none and cosine\n",
    "                        for (schedul, er) in schedulers.items():\n",
    "                            print(hidden_size, epochs, init,\n",
    "                                active, optimi, schedul, \"start\")\n",
    "                            # train(model=model, optimizer=zer, device=device, epochs=epochs, learning_rate=learning_rate,\n",
    "                            #       trainloader=trainloader, valloader=valloader, criterion=criterion, scheduler=er)\n",
    "                            print(hidden_size, epochs, init,\n",
    "                                active, optimi, schedul, \"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.7292, Train Accuracy: 67.26%, Val Loss: 0.6387, Val Accuracy: 66.56%\n",
      "Epoch [2/10], Train Loss: 0.6338, Train Accuracy: 67.53%, Val Loss: 0.6373, Val Accuracy: 66.56%\n",
      "Epoch [3/10], Train Loss: 0.6333, Train Accuracy: 67.53%, Val Loss: 0.6378, Val Accuracy: 66.56%\n",
      "Epoch [4/10], Train Loss: 0.6335, Train Accuracy: 67.53%, Val Loss: 0.6433, Val Accuracy: 66.56%\n",
      "Epoch [5/10], Train Loss: 0.6341, Train Accuracy: 67.53%, Val Loss: 0.6405, Val Accuracy: 66.56%\n",
      "Epoch [6/10], Train Loss: 0.6339, Train Accuracy: 67.53%, Val Loss: 0.6383, Val Accuracy: 66.56%\n",
      "Epoch [7/10], Train Loss: 0.6344, Train Accuracy: 67.53%, Val Loss: 0.6385, Val Accuracy: 66.56%\n",
      "Epoch [8/10], Train Loss: 0.6355, Train Accuracy: 67.53%, Val Loss: 0.6475, Val Accuracy: 66.56%\n",
      "Epoch [9/10], Train Loss: 0.6335, Train Accuracy: 67.53%, Val Loss: 0.6379, Val Accuracy: 66.56%\n",
      "Epoch [10/10], Train Loss: 0.6338, Train Accuracy: 67.53%, Val Loss: 0.6375, Val Accuracy: 66.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set device to use (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = len(trainset[0][0])\n",
    "hidden_size = 11\n",
    "output_size = 2\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = TwoLayerNetwork(input_size, hidden_size, output_size, inits[\"kaiming\"], actives[\"relu\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train(model, optims[\"adam\"], device, num_epochs, learning_rate, trainloader, valloader, criterion, schedulers[\"cos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0725737be4be03859ccf648c604bdce2d511d4addba95219b9055f0ea318ae44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
